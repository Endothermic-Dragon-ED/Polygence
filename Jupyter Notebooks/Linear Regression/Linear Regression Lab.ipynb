{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idl1fksiUoj0"
      },
      "source": [
        "# Linear Regression Lab\n",
        "## Pre-lab\n",
        "In reality, before applying a machine learning algorithm, you would need to:\n",
        "1. Find a dataset\n",
        "2. Modify the dataset as per your needs\n",
        "3. Preprocess the dataset for improved performance (normalize, reduce dimensions, etc.)\n",
        "For now, the first two steps have already been done for you, and the third step will be discussed in the next article.\n",
        "\n",
        "To import the data and any necessary libraries, simply run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YP8leGBUoj1"
      },
      "outputs": [],
      "source": [
        "# Deal with large arrays quickly and easily\n",
        "import numpy as np\n",
        "# Display data table\n",
        "import pandas as pd\n",
        "\n",
        "# Get data from CSV file on GitHub\n",
        "dataTable = pd.read_csv(\"https://raw.githubusercontent.com/Endothermic-Dragon/Polygence/master/Jupyter%20Notebooks/Linear%20Regression/Fish%20Dataset.csv\")\n",
        "\n",
        "# Get data as array\n",
        "data = dataTable.to_numpy()\n",
        "\n",
        "# Display data table\n",
        "dataTable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U9Y0m7cUoj1"
      },
      "source": [
        "After running the cell above, you should be able to see a data table about fish. Your goal is to take the fish species, vertical length, diagonal length, cross length, height, and diagonal width, and use that to predict the fish's weight. First, let's perform a basic visualization of the data to see what kind of model would work well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsI2W_KvUoj2"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Specify five subplots\n",
        "fig, axes = plt.subplots(5, 1, figsize=(5, 25))\n",
        "\n",
        "# Generate and assign subplots\n",
        "sns.scatterplot(ax=axes[0], data=dataTable, x=\"Vertical Length (cm)\", y=\"Weight (g)\")\n",
        "sns.scatterplot(ax=axes[1], data=dataTable, x=\"Diagonal Length (cm)\", y=\"Weight (g)\")\n",
        "sns.scatterplot(ax=axes[2], data=dataTable, x=\"Cross Length (cm)\", y=\"Weight (g)\")\n",
        "sns.scatterplot(ax=axes[3], data=dataTable, x=\"Height (cm)\", y=\"Weight (g)\")\n",
        "sns.scatterplot(ax=axes[4], data=dataTable, x=\"Diagonal Width (cm)\", y=\"Weight (g)\")\n",
        "\n",
        "# Delete temporary variables\n",
        "del fig, axes\n",
        "\n",
        "# Show figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7JoUb82Uoj2"
      },
      "source": [
        "Above, we have analyzed the direct relationships between our inputs and outputs. Note that the relations could be multivariable, but the dataset has been chosen so it is not, as the purpose of this exercise is to learn the inner computational mechanics of linear regression.\n",
        "\n",
        "It seems like each of the input columns would benefit from a fit with a squared term. So, let's add that into our linear regression equation to reduce bias.\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$f(\\theta, x_i)=\\theta_1 x_{i,\\, \\text{vertical\\_length}} + \\theta_2 x_{i,\\, \\text{vertical\\_length}}^2 \\\\ + \\theta_3 x_{i,\\, \\text{diagonal\\_length}} + \\theta_4 x_{i,\\, \\text{diagonal\\_length}}^2 \\\\ + \\theta_5 x_{i,\\, \\text{cross\\_length}} + \\theta_6 x_{i,\\, \\text{cross\\_length}}^2 \\\\ + \\theta_7 x_{i,\\, \\text{height}} + \\theta_8 x_{i,\\, \\text{height}}^2 \\\\ + \\theta_9 x_{i,\\, \\text{diagonal\\_width}} + \\theta_{10} x_{i,\\, \\text{diagonal\\_width}}^2 \\\\ + \\theta_{11}$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "Note that we will preprocess the squared terms and store them in an array to reduce the computation necessary in the next cell. In addition, we will be also splitting the data into training and validation groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr8jWzAWUoj2"
      },
      "outputs": [],
      "source": [
        "# Shuffle data to make it unordered and random\n",
        "np.random.shuffle(data)\n",
        "\n",
        "trainingData = data[:71, :]\n",
        "validationData = data[71:, :]\n",
        "\n",
        "# Get columns\n",
        "x_vertical_length, x_diagonal_length, x_cross_length, x_height, x_diagonal_width, trainingOutputs = np.hsplit(trainingData, 6)\n",
        "\n",
        "# Horizontally stack columns and additional columns as appropriate\n",
        "trainingData = np.hstack((\n",
        "    x_vertical_length, x_vertical_length**2,\n",
        "    x_diagonal_length, x_diagonal_length**2,\n",
        "    x_cross_length, x_cross_length**2,\n",
        "    x_height, x_height**2,\n",
        "    x_diagonal_width, x_diagonal_width**2\n",
        "))\n",
        "\n",
        "# Get columns\n",
        "x_vertical_length, x_diagonal_length, x_cross_length, x_height, x_diagonal_width, validationOutputs = np.hsplit(validationData, 6)\n",
        "\n",
        "# Horizontally stack columns and additional columns as appropriate\n",
        "validationData = np.hstack((\n",
        "    x_vertical_length, x_vertical_length**2,\n",
        "    x_diagonal_length, x_diagonal_length**2,\n",
        "    x_cross_length, x_cross_length**2,\n",
        "    x_height, x_height**2,\n",
        "    x_diagonal_width, x_diagonal_width**2\n",
        "))\n",
        "\n",
        "# Remove temporary variables\n",
        "del x_vertical_length, x_diagonal_length, x_cross_length, x_height, x_diagonal_width\n",
        "\n",
        "print(\"Data split successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWrfQgPlUoj2"
      },
      "source": [
        "This lab also contains various checks to make sure that you've implemented the functions correctly. Run the next cell to initialize them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImWdRQELUoj3"
      },
      "outputs": [],
      "source": [
        "def checkF():\n",
        "    valid = []\n",
        "    for _ in range(5):\n",
        "        # Randomly generate \"data\"\n",
        "        thetas = np.random.uniform(-10, 10, 11)\n",
        "        xs = np.random.uniform(-10, 10, 10)\n",
        "\n",
        "        # Get the correct answer\n",
        "        expectedOutputs = sum(thetas[i]*xs[i] for i in range(10))+ thetas[10]\n",
        "\n",
        "        # Compare function's output to the correct answer\n",
        "        # Add up the absolute value of the differences\n",
        "        # Make sure that the sum is under the float error tolerance of 0.01\n",
        "        validity = np.sum(np.abs(f(thetas, xs) - expectedOutputs)) < 0.01\n",
        "\n",
        "        # Add validity check to list\n",
        "        valid.append(validity)\n",
        "\n",
        "    # If all checks are true, return success\n",
        "    if all(valid):\n",
        "        print(\"Successful implementation of function!\")\n",
        "    else:\n",
        "        print(\"Oops! There's an error in your code.\")\n",
        "\n",
        "def checkJ():\n",
        "    valid = []\n",
        "    for _ in range(5):\n",
        "        # Randomly generate \"data\"\n",
        "        thetas = np.random.uniform(-10, 10, 11)\n",
        "        data = np.random.uniform(-10, 10, (100, 10))\n",
        "        outputs = np.random.uniform(-10, 10, (100, 1))\n",
        "\n",
        "        # Get the correct answer\n",
        "        expectedOutputs = np.mean(\n",
        "            (\n",
        "                (np.hstack((data, np.ones((100, 1)))) @ np.array([thetas]).T) - outputs\n",
        "            )**2\n",
        "        )\n",
        "\n",
        "        # Compare function's output to the correct answer\n",
        "        # Add up the absolute value of the differences\n",
        "        # Make sure that the sum is under the float error tolerance of 0.01\n",
        "        validity = np.sum(np.abs(J(thetas, data, outputs) - expectedOutputs)) < 0.1\n",
        "\n",
        "        # Add validity check to list\n",
        "        valid.append(validity)\n",
        "\n",
        "    # If all checks are true, return success\n",
        "    if all(valid):\n",
        "        print(\"Successful implementation of function!\")\n",
        "    else:\n",
        "        print(\"Oops! There's an error in your code.\")\n",
        "\n",
        "def checkGradients():\n",
        "    valid = []\n",
        "    for _ in range(5):\n",
        "        # Randomly generate \"data\"\n",
        "        thetas = np.random.uniform(-10, 10, 11)\n",
        "        data = np.random.uniform(-10, 10, (100, 10))\n",
        "        outputs = np.random.uniform(-10, 10, (100, 1))\n",
        "\n",
        "        # Get the correct answer\n",
        "        expectedOutputs = np.mean(\n",
        "            (\n",
        "                (np.hstack((data, np.ones((100, 1)))) @ np.array([thetas]).T) - outputs\n",
        "            ) * np.hstack((data, np.ones((100, 1))))\n",
        "        , 0) * 2\n",
        "\n",
        "        # Compare function's output to the correct answer\n",
        "        # Add up the absolute value of the differences\n",
        "        # Make sure that the sum is under the float error tolerance of 0.01\n",
        "        validity = np.sum(np.abs(getGradients(thetas, data, outputs) - expectedOutputs)) < 0.1\n",
        "\n",
        "        # Add validity check to list\n",
        "        valid.append(validity)\n",
        "\n",
        "    # If all checks are true, return success\n",
        "    if all(valid):\n",
        "        print(\"Successful implementation of function!\")\n",
        "    else:\n",
        "        print(\"Oops! There's an error in your code.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0edmBmLVUoj3"
      },
      "source": [
        "\n",
        "Great! Now you have everything you need to make your linear regression model. Specifically, you have to use these to train your model:\n",
        "* `trainingData` - data which has all the training input data, with the columns in this order:\n",
        "  * $\\text{Vertical Length}$\n",
        "  * $\\text{Diagonal Length}^2$\n",
        "  * $\\text{Diagonal Length}$\n",
        "  * $\\text{Diagonal Length}^2$\n",
        "  * $\\text{Cross Length}$\n",
        "  * $\\text{Cross Length}^2$\n",
        "  * $\\text{Height}$\n",
        "  * $\\text{Height}^2$\n",
        "  * $\\text{Diagonal Width}$\n",
        "  * $\\text{Diagonal Width}^2$\n",
        "* `validationData` - data which has all the validation input data in the same column layout as above\n",
        "* `trainingOutputs` - a column of the expected outputs for the corresponding training data\n",
        "* `validaitonOutputs` - a column of the expected outputs for the corresponding validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmVOEBEUoj3"
      },
      "source": [
        "But wait, one more thing before you start the hands-on portion:\n",
        "## Quick Tutorial\n",
        "\n",
        "How the heck do you process all of this data without descending into `for` loop hell? Well, `numpy` is a python library that's used to process large amounts of data in an easy manner. One of its advantages is that you don't need to write a loop to process each row of data. You'll need to know the following functions for this lab:\n",
        "* `np.array` - converts a native python array to a numpy array\n",
        "* `np_array[a:b, c:d]` - gets elements from row a (inclusive) to b (exclusive), and then column c (inclusive) to d (exclusive). This is very similar to accessing elements in a python list, but now you can access more than one dimension at once.\n",
        "* `np_array + np_array` - adds the two arrays element-wise. For example, `[1, 2] + [3, 4] = [4, 6]`.\n",
        "* `np_array - np_array` - subtracts the two arrays element-wise. For example, `[1, 2] - [3, 4] = [-2, -2]`.\n",
        "* `np_array * np_array` - multiplies the two arrays element-wise. For example, `[1, 2] * [3, 4] = [3, 8]`.\n",
        "* `np.mean(np_array)` - gives the average value for all the cells.\n",
        "\n",
        "Instead of importing `math` and using `math.pow`, you can use `**` as its equivalent. **MAKE SURE** that you don't use `^`, as that is a bitwise OR operator, not a math exponent. If you don't know what that is, don't worry about it, just remember to not use it.  \n",
        "Example: `3**4` returns `81`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR1ZgnKKUoj4"
      },
      "source": [
        "## Programming the Model\n",
        "First, implement the $f(\\theta, x_i)$ function. Remember that $x_i$ contains 10 values in the format of a row, and $\\theta$ contains 11 values in the format of a row.\n",
        "\n",
        "Running the cell will automatically tell you whether your implementation is correct or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKqGw9PVUoj4"
      },
      "outputs": [],
      "source": [
        "def f(thetas, xs):\n",
        "    return\n",
        "\n",
        "checkF()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDArpDJ8Uoj4"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example function that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "def f(thetas, xs):\n",
        "    # Multiply corresponding thetas to xs and add them together\n",
        "    result = 0\n",
        "    for i in range(10):\n",
        "        result += thetas[i]*xs[i]\n",
        "    \n",
        "    # Add the last theta and return result\n",
        "    return result + thetas[10]\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeF9j8bJUoj4"
      },
      "source": [
        "You've implemented your function. Great job! Now, you have to implement the cost function. We will graph this over iterations to ensure that we're making progress. Here's the cost function formula:\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$J(θ) = \\frac{1}{n} \\sum_{i=1}^n (f(θ, x_i) - y_i)^2$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "In the function below, you can see that the parameters `data` and `outputs` are initialized default values of `trainingData` and `trainingOutputs`, respectively, if they are not specified when calling the function. This will come in handy later, when we'll be checking for overfitting on our validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUH06ascUoj4"
      },
      "outputs": [],
      "source": [
        "def J(thetas, data=trainingData, outputs=trainingOutputs):\n",
        "    return\n",
        "\n",
        "checkJ()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw2Ih4ohUoj5"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example function that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "def J(thetas, data=trainingData, outputs=trainingOutputs):\n",
        "    result = 0\n",
        "    # Loop through each row, adding results together\n",
        "    for i in range(len(data)):\n",
        "        # Two arrays go into f, number returned\n",
        "        # Subtract prediction and expected output, square, and add to result\n",
        "        result += (f(thetas, data[i]) - outputs[i, 0])**2\n",
        "    # Take the average and return\n",
        "    return result/len(data)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8oqq37KUoj5"
      },
      "source": [
        "Next, it's time to implement the gradients. If you don't know how to take the gradient of the cost function, then click the dropdowns below to reveal them.\n",
        "\n",
        "<details>\n",
        "<summary>View Gradients</summary>\n",
        "<h3>\n",
        "<details>\n",
        "<summary>\n",
        "\n",
        "$\\frac{\\partial}{\\partial \\theta_1} J(\\theta)$</summary>\n",
        "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{vertical\\_length}})$$\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>\n",
        "\n",
        "$\\frac{\\partial}{\\partial \\theta_2} J(\\theta)$</summary>\n",
        "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{vertical\\_length}}^2)$$\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>\n",
        "\n",
        "$\\frac{\\partial}{\\partial \\theta_3} J(\\theta)$</summary>\n",
        "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{diagonal\\_length}})$$\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>\n",
        "\n",
        "$\\frac{\\partial}{\\partial \\theta_4} J(\\theta)$</summary>\n",
        "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{diagonal\\_length}}^2)$$\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>\n",
        "\n",
        "$\\frac{\\partial}{\\partial \\theta_5} J(\\theta)$</summary>\n",
        "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{cross\\_length}})$$\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>\n",
        "\n",
        "$\\frac{\\partial}{\\partial \\theta_6} J(\\theta)$</summary>\n",
        "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{cross\\_length}}^2)$$\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>\n",
        "\n",
        "$\\frac{\\partial}{\\partial \\theta_7} J(\\theta)$</summary>\n",
        "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{height}})$$\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>\n",
        "\n",
        "$\\frac{\\partial}{\\partial \\theta_8} J(\\theta)$</summary>\n",
        "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{height}}^2)$$\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>\n",
        "\n",
        "$\\frac{\\partial}{\\partial \\theta_9} J(\\theta)$</summary>\n",
        "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{diagonal\\_width}})$$\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>\n",
        "\n",
        "$\\frac{\\partial}{\\partial \\theta_{10}} J(\\theta)$</summary>\n",
        "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{diagonal\\_width}}^2)$$\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>\n",
        "\n",
        "$\\frac{\\partial}{\\partial \\theta_{11}} J(\\theta)$</summary>\n",
        "$$\\frac{2}{n} \\sum_{i=1}^n (f(θ, x_i) - y_i)$$\n",
        "</details>\n",
        "</h3>\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Implementation Hint</summary>\n",
        "\n",
        "Try to use the fact that all the partial derivatives have $f(θ, x_i) - y_i$ and $\\frac{2}{n}$ in common to \"parallel process\" these.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFt09BJuUoj5"
      },
      "outputs": [],
      "source": [
        "def getGradients(thetas, data=trainingData, outputs=trainingOutputs):\n",
        "    return\n",
        "\n",
        "checkGradients()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEzU2MaNUoj5"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example function that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "def getGradients(thetas, data=trainingData, outputs=trainingOutputs):\n",
        "    # Get length of training data\n",
        "    n = len(data)\n",
        "\n",
        "    # Store gradient values\n",
        "    results = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "    # Loop through each training datapoint\n",
        "    for i in range(n):\n",
        "        # Calculate the part which all the gradients have in common\n",
        "        resultPart = f(thetas, data[i]) - outputs[i, 0]\n",
        "        for j in range(10):\n",
        "            # Multiply the \"uncommon\" part for each and add to the sum\n",
        "            results[j] += resultPart * data[i, j]\n",
        "        results[10] += resultPart\n",
        "\n",
        "    # Multiply each sum by 2/n to get the gradient for each theta\n",
        "    for i in range(11):\n",
        "        results[i] = 2 * results[i] / n\n",
        "\n",
        "    return results\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_OWcl6KUoj6"
      },
      "source": [
        "Finally, it's time to implement a complete iteration that takes in $\\theta$ values, and returns new $\\theta$ values after performing a gradient descent step with step size of 0.0000003.\n",
        "\n",
        "If you're wondering why it's such a small and weird number, it's because that's the value that works well in this scenario, as found by trial and error. To do this yourself, simply print the gradients and see if they're converging to a minimum efficiently.\n",
        "\n",
        "At this point, you're probably thinking why we don't use 0.1, 0.001, or one of those other values we went over in the Gradient Descent article. These learning rates are determined as \"reasonably good\" after a process called regularization or normalization, which speeds up the training process significantly. We will go over how to normalization data in the next article.\n",
        "\n",
        "When editing this next cell, keep in mind whether you implemented the `getGradients` function to return a python list or a numpy array.\n",
        "\n",
        "This should be relatively easy, so there isn't a check for this part. You can just compare your code with the example code after you're done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2l2C0B5Uoj6"
      },
      "outputs": [],
      "source": [
        "def step(thetas):\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCacza2ZUoj6"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example function that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "def step(thetas):\n",
        "    # Get the gradients\n",
        "    gradients = getGradients(thetas)\n",
        "\n",
        "    # Loop through and update each theta\n",
        "    for i in range(11):\n",
        "        thetas[i] = thetas[i] - 0.0000003*gradients[i]\n",
        "    \n",
        "    return thetas\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp05iJK4Uoj6"
      },
      "source": [
        "Now that you're done implementing all of the functions, run the next cell to perform 500 iterations, and see how the cost function goes down.\n",
        "\n",
        "Feel free to adjust the number of iterations, stored in the variable `iterations`, and see how the cost changes over time. Even if you change it to 5, you should be able to see how the cost dramatically goes down, showing how effective this algorithm is!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whhpgvJuUoj6"
      },
      "outputs": [],
      "source": [
        "# Number of iterations to run\n",
        "iterations = 0\n",
        "\n",
        "# Set any values for theta\n",
        "thetas = np.zeros(11)\n",
        "\n",
        "# Store cost history for plotting later\n",
        "cost_history = [J(thetas)]\n",
        "\n",
        "# Run iterations - perform a step + store cost in history for plotting later\n",
        "for _ in range(iterations):\n",
        "    thetas = step(thetas)\n",
        "    cost_history.append(J(thetas))\n",
        "\n",
        "print(\"Starting cost:\", cost_history[0])\n",
        "print(\"Ending cost:\", cost_history[-1])\n",
        "\n",
        "# Plot cost\n",
        "plt.figure()\n",
        "plt.plot(np.arange(iterations+1), cost_history)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaNgKv7sUoj6"
      },
      "source": [
        "But wait, we're not done! We still need to use out validation data. We'll plot the cost on the validation dataset over each iteration, in order to ensure that the cost is going down and we aren't overfitting.\n",
        "\n",
        "## Model Validation\n",
        "\n",
        "Remember how we had parameters `data` and `outputs` in our `J` function? It's time to reuse those, by calling the function with `validationData` and `validationOutputs`.\n",
        "\n",
        "Using the code above, implement a similar version that measures the cost using the validation data over iterations. Remember that you only want to change the dataset for which the cost is measured, not train it for that new dataset, because you want to make sure that your model generalizes well.\n",
        "\n",
        "The code above has been copied below for your convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXX3Ztj0Uoj6"
      },
      "outputs": [],
      "source": [
        "# Number of iterations to run\n",
        "iterations = 0\n",
        "\n",
        "# Set any values for theta\n",
        "thetas = np.zeros(11)\n",
        "\n",
        "# Store cost history for plotting later\n",
        "cost_history = [J(thetas)]\n",
        "\n",
        "# Run iterations - perform a step + store cost in history for plotting later\n",
        "for _ in range(iterations):\n",
        "    thetas = step(thetas)\n",
        "    cost_history.append(J(thetas))\n",
        "\n",
        "print(\"Starting cost:\", cost_history[0])\n",
        "print(\"Ending cost:\", cost_history[-1])\n",
        "\n",
        "# Plot cost\n",
        "plt.figure()\n",
        "plt.plot(np.arange(iterations+1), cost_history)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrabou50Uoj7"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example function that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "# Number of iterations to run\n",
        "iterations = 1000\n",
        "\n",
        "# Set any values for theta\n",
        "thetas = np.zeros(11)\n",
        "\n",
        "# Store cost history for plotting later\n",
        "cost_history = [J(thetas)]\n",
        "\n",
        "# Run iterations - perform a step + store cost in history for plotting later\n",
        "for _ in range(iterations):\n",
        "    thetas = step(thetas)\n",
        "    # Modified here - pass validation data and outputs to the cost function J\n",
        "    cost_history.append(J(thetas, validationData, validationOutputs))\n",
        "\n",
        "print(\"Starting cost:\", cost_history[0])\n",
        "print(\"Ending cost:\", cost_history[-1])\n",
        "\n",
        "# Plot cost\n",
        "plt.figure()\n",
        "plt.plot(np.arange(iterations+1), cost_history)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kSW4k_plube"
      },
      "source": [
        "As you can see, this has a cost value similar to that of our training data, showing that our data does not contain significant variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf7nQa0ZUoj7"
      },
      "source": [
        "## Model Accuracy\n",
        "Great! Now you know for sure that your model works. But how well? We can measure that using something called the coefficient of determination, or $R^2$ for short. This provides a measure for how good of a fit your model is, and is calculated by the following formula:\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$R^2 = 1-\\frac{\\sum_{i=1}^n (f(\\theta, x_i) - y_i)^2}{\\sum_{i=1}^n (y_{average} - y_i)^2}$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "The value of $R^2$ always falls between 0 and 1, where 1 represents a perfect fit. We can use this to judge the accuracy of our model as a percentage.\n",
        "\n",
        "Implement the formula above using the validation data to quantify the accuracy level of our model in the next cell. You should have around a 90% accuracy, but there will be fluctuations due to randomization in the splitting of the original data into training and validation sets.\n",
        "\n",
        "<details>\n",
        "<summary>Implementation Hint</summary>\n",
        "\n",
        "Using the `np.mean` function might be helpful.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVUEB7D4Uoj7"
      },
      "outputs": [],
      "source": [
        "# Value of thetas carry over from previous cell\n",
        "\n",
        "r_squared = 0\n",
        "\n",
        "# Type your code here\n",
        "# The value of r_squared should be a decimal between 0 and 1 after calculations\n",
        "\n",
        "\n",
        "# Display R^2 as percentage\n",
        "print(f\"{'%.2f' % (r_squared*100)}% accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GuQsGOdUoj7"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example function that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "# Value of thetas carry over from previous cell\n",
        "\n",
        "# Get number of rows\n",
        "n = len(validationData)\n",
        "\n",
        "# Get the mean of the output values\n",
        "averageValidationOutput = np.mean(validationOutputs)\n",
        "\n",
        "# Calculate numerator and denominator sum with loop\n",
        "numerator = 0\n",
        "denominator = 0\n",
        "\n",
        "for i in range(n):\n",
        "    numerator += (f(thetas, validationData[i]) - validationOutputs[i, 0])**2\n",
        "    denominator += (averageValidationOutput - validationOutputs[i, 0])**2\n",
        "\n",
        "# Plug into formula\n",
        "r_squared = 1 - numerator/denominator\n",
        "\n",
        "# Display as percentage\n",
        "print(f\"{'%.2f' % (r_squared*100)}% accuracy\")\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysdElIu1m9s5"
      },
      "source": [
        "As a side note, you can increase the accuracy by running more iterations, implementing a better learning rate constant, implementing a dynamic learning rate as was discussed in the article, or using a more advanced gradient descent algorithm.\n",
        "\n",
        "Running 10,000 iterations should give you around 96% accuracy, and running 100,000 iterations should give you a little higher than 97.5% accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8PepoXVUoj7"
      },
      "source": [
        "Congratuations, you've now successfully completed the lab! But, as you can tell, that was a lot of work. We can make these calculations a lot easier and faster by utilizing matrix computations and numpy arrays to our advantage, which we will learn about in the next article."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA6I9uLBUoj7"
      },
      "source": [
        "## Credits\n",
        "* This lab used a modified version of Aung Pyae's fish market dataset from Kaggle. You can find the original dataset [here](https://www.kaggle.com/datasets/aungpyaeap/fish-market).\n",
        "* Formula to calculate $R^2$ accuracy from [Newcastle University, UK](https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/coefficient-of-determination-r-squared.html)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ce9e5fc50149dfb09ca60d84ae2680dbf18b569ea9aa8de02c26aa2ebff8fc36"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
