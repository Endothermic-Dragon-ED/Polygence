{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not updated, latest version on Google Colab: https://colab.research.google.com/drive/12tqB8YPZ2aXZ49swd0iLn_PEuOc7AdLv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Lab\n",
    "### Pre-lab\n",
    "In reality, before applying a machine learning algorithm, you would need to:\n",
    "1. Find a dataset\n",
    "2. Modify the dataset as per your needs\n",
    "3. Preprocess the dataset for improved performance (normalize, reduce dimensions, etc.)\n",
    "For now, the first two steps have already been done for you, and the third step will be discussed in the next article.\n",
    "\n",
    "To import the data and any necessary libraries, simply run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with large arrays quickly and easily\n",
    "import numpy as np\n",
    "# Display data table\n",
    "import pandas as pd\n",
    "\n",
    "# Get data from CSV file\n",
    "with open(\"Fish Dataset.csv\") as f:\n",
    "    titlesRaw, dataRaw = f.read().split(\"\\n\", 1)\n",
    "    titles = titlesRaw.split(\",\")\n",
    "    data = np.array([i.split(\",\") for i in dataRaw.split(\"\\n\")]).astype(float)\n",
    "del titlesRaw, dataRaw, f\n",
    "\n",
    "# Create data table\n",
    "dataTable = pd.DataFrame(data, columns=titles)\n",
    "\n",
    "# Display data table\n",
    "dataTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the cell above, you should be able to see a data table about fish. Your goal is to take the fish species, vertical length, diagonal length, cross length, height, and diagonal width, and use that to predict the fish's weight. First, let's perform a basic visualization of the data to see what kind of model would work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(5, 25))\n",
    "\n",
    "sns.scatterplot(ax=axes[0], data=dataTable, x=\"Vertical Length (cm)\", y=\"Weight (g)\")\n",
    "sns.scatterplot(ax=axes[1], data=dataTable, x=\"Diagonal Length (cm)\", y=\"Weight (g)\")\n",
    "sns.scatterplot(ax=axes[2], data=dataTable, x=\"Cross Length (cm)\", y=\"Weight (g)\")\n",
    "sns.scatterplot(ax=axes[3], data=dataTable, x=\"Height (cm)\", y=\"Weight (g)\")\n",
    "sns.scatterplot(ax=axes[4], data=dataTable, x=\"Diagonal Width (cm)\", y=\"Weight (g)\")\n",
    "\n",
    "del fig, axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have analyzed the direct relationships between our inputs and outputs. Note that the relations could be multivariable, but the dataset has been chosen so it is not, as the purpose of this exercise is to learn the inner computational mechanics of linear regression, not how to apply it.\n",
    "\n",
    "It seems like each of the parameters would benefit from a fit with a squared term. So, let's add that into our linear regression equation.\n",
    "\n",
    "$$f(\\theta, x_i)=\\theta_1 x_{i,\\, \\text{vertical\\_length}} + \\theta_2 x_{i,\\, \\text{vertical\\_length}}^2$$  \n",
    "$$+ \\theta_3 x_{i,\\, \\text{diagonal\\_length}} + \\theta_4 x_{i,\\, \\text{diagonal\\_length}}^2$$  \n",
    "$$+ \\theta_5 x_{i,\\, \\text{cross\\_length}} + \\theta_6 x_{i,\\, \\text{cross\\_length}}^2$$  \n",
    "$$+ \\theta_7 x_{i,\\, \\text{height}} + \\theta_8 x_{i,\\, \\text{height}}^2$$  \n",
    "$$+ \\theta_9 x_{i,\\, \\text{diagonal\\_width}} + \\theta_{10} x_{i,\\, \\text{diagonal\\_width}}^2$$  \n",
    "$$+ \\theta_{11}$$\n",
    "\n",
    "Note that we will preprocess the squared terms and store them in an array to reduce the computation necessary in the next cell. In addition, we will be also splitting the data into training and validation groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data to make it unordered and random\n",
    "np.random.shuffle(data)\n",
    "\n",
    "trainingData = data[:71, :]\n",
    "validationData = data[71:, :]\n",
    "\n",
    "x_vertical_length, x_diagonal_length, x_cross_length, x_height, x_diagonal_width, trainingOutputs = np.hsplit(trainingData, 6)\n",
    "trainingData = np.hstack((\n",
    "    x_vertical_length, x_vertical_length**2,\n",
    "    x_diagonal_length, x_diagonal_length**2,\n",
    "    x_cross_length, x_cross_length**2,\n",
    "    x_height, x_height**2,\n",
    "    x_diagonal_width, x_diagonal_width**2\n",
    "))\n",
    "\n",
    "x_vertical_length, x_diagonal_length, x_cross_length, x_height, x_diagonal_width, validationOutputs = np.hsplit(validationData, 6)\n",
    "validationData = np.hstack((\n",
    "    x_vertical_length, x_vertical_length**2,\n",
    "    x_diagonal_length, x_diagonal_length**2,\n",
    "    x_cross_length, x_cross_length**2,\n",
    "    x_height, x_height**2,\n",
    "    x_diagonal_width, x_diagonal_width**2\n",
    "))\n",
    "\n",
    "del x_vertical_length, x_diagonal_length, x_cross_length, x_height, x_diagonal_width\n",
    "\n",
    "print(\"Data split successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab contains various checks to make sure that you've implemented the functions correctly. Run the next cell to initialize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkF():\n",
    "    check1 = f(np.array([1,2,3,4,5,6,7,8,9,10,11]).T, np.array([1,2,3,4,5,6,7,8,9,10]).T) == 396\n",
    "    check2 = f(np.array([1,2,3,4,5,6,7,8,9,10,11]).T, np.array([10,9,8,7,6,5,4,3,2,1]).T) == 231\n",
    "\n",
    "    if check1 and check2:\n",
    "        print(\"Successful implementation of function!\")\n",
    "    else:\n",
    "        print(\"Oops! There's an error in your code.\")\n",
    "\n",
    "def checkJ():\n",
    "    global trainingData, trainingOutputs\n",
    "\n",
    "    temp1 = trainingData\n",
    "    temp2 = trainingOutputs\n",
    "\n",
    "    trainingData = np.array([[3, 5, 2, 5, 7, 3, 5, 7, 9, 1], [7, 2, 7, 9, 12, 3, -2, 9, 1, 100]])\n",
    "    trainingOutputs = np.array([[71], [23]])\n",
    "\n",
    "    check1 = 2784 < J(np.array([0,0,0,0,0,0,0,0,0,0,0])) < 2786\n",
    "    check2 = 8201 < J(np.array([1,1,1,1,1,1,1,1,1,1,1])) < 8203\n",
    "\n",
    "    trainingData = temp1\n",
    "    trainingOutputs = temp2\n",
    "\n",
    "    if check1 and check2:\n",
    "        print(\"Successful implementation of function!\")\n",
    "    else:\n",
    "        print(\"Oops! There's an error in your code.\")\n",
    "\n",
    "def checkGradients():\n",
    "    global trainingData, trainingOutputs\n",
    "\n",
    "    temp1 = trainingData\n",
    "    temp2 = trainingOutputs\n",
    "\n",
    "    trainingData = np.array([[3, 5, 2, 5, 7, 3, 5, 7, 9, 1], [7, 2, 7, 9, 12, 3, -2, 9, 1, 100]])\n",
    "    trainingOutputs = np.array([[71], [23]])\n",
    "\n",
    "    check1 = getGradients(np.array([0,0,0,0,0,0,0,0,0,0,0]))\n",
    "    check2 = getGradients(np.array([1,1,1,1,1,1,1,1,1,1,1]))\n",
    "\n",
    "    expectedCheck1 = [-374.0, -401.0, -303.0, -562.0, -773.0, -282.0, -309.0, -704.0, -662.0, -2371.0, -94.0]\n",
    "    expectedCheck2 = [813.0, 137.0, 836.0, 1019.0, 1351.0, 309.0, -367.0, 973.0, -81.0, 12577.0, 103.0]\n",
    "\n",
    "    correct = True\n",
    "    for i in range(11):\n",
    "        correct = correct and expectedCheck1[i]-1.5 < check1[i] < expectedCheck1[i]+1.5 and\\\n",
    "                    expectedCheck2[i]-1.5 < check2[i] < expectedCheck2[i]+1.5\n",
    "\n",
    "    trainingData = temp1\n",
    "    trainingOutputs = temp2\n",
    "\n",
    "    if correct:\n",
    "        print(\"Successful implementation of function!\")\n",
    "    else:\n",
    "        print(\"Oops! There's an error in your code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Great! Now you have everything you need to make your linear regression model. Specifically, you have to use these to train your model:\n",
    "* `trainingData` - data which has all the training input data, with the columns in this order:\n",
    "  * $\\text{Vertical Length}$\n",
    "  * $\\text{Diagonal Length}^2$\n",
    "  * $\\text{Diagonal Length}$\n",
    "  * $\\text{Diagonal Length}^2$\n",
    "  * $\\text{Cross Length}$\n",
    "  * $\\text{Cross Length}^2$\n",
    "  * $\\text{Height}$\n",
    "  * $\\text{Height}^2$\n",
    "  * $\\text{Diagonal Width}$\n",
    "  * $\\text{Diagonal Width}^2$\n",
    "* `validationData` - data which has all the validation input data in the same column layout as above\n",
    "* `trainingOutputs` - a column of the expected outputs for the corresponding training data\n",
    "* `validaitonOutputs` - a column of the expected outputs for the corresponding validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait, one more thing before you start the hands-on portion:\n",
    "### Quick Tutorial\n",
    "\n",
    "How the heck do you process all of this data without descending into `for` loop hell? Well, `numpy` is a python library that's used to process large amounts of data in an easy manner. One of its advantages is that you don't need to write a loop to process each row of data. You'll need to use the following functions for this lab:\n",
    "* `np.array` - converts a native python array to a numpy array\n",
    "* `np_array[a:b, c:d]` - gets elements from row a (inclusive) to b (exclusive), and then column c (inclusive) to d (exclusive). This is very similar to accessing elements in a python list, but now you can access more than one dimension at once.\n",
    "* `np_array + np_array` - adds the two arrays element-wise. For example, `[1, 2] + [3, 4] = [4, 6]`.\n",
    "* `np_array - np_array` - subtracts the two arrays element-wise. For example, `[1, 2] - [3, 4] = [-2, -2]`.\n",
    "* `np_array * np_array` - multiplies the two arrays element-wise. For example, `[1, 2] * [3, 4] = [3, 8]`.\n",
    "\n",
    "Instead of math.pow, you can use `**` as its equivalent. **MAKE SURE** that you don't use `^`, as that is a bitwise OR operator, not a math exponent. If you don't know what that is, don't worry about it, just remember to not use it.  \n",
    "Example: `3**4` returns `81`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-On Section\n",
    "First, implement the $f(\\theta, x_i)$ function. Remember that $x_i$ contains 10 values in the format of a row, and $\\theta$ contains 11 values in the format of a row.\n",
    "\n",
    "Running the cell will automatically tell you whether your implementation is correct or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(thetas, xs):\n",
    "    # Type your code here\n",
    "    return sum(thetas[i]*xs[i] for i in range(10))+ thetas[10]\n",
    "\n",
    "checkF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Stuck or completed? Click here to reveal a working example function that you could've written.</summary>\n",
    "\n",
    "```py\n",
    "def f(thetas, xs):\n",
    "    # Multiply corresponding thetas to xs and add them together\n",
    "    result = 0\n",
    "    for i in range(10):\n",
    "        result += thetas[i]*xs[i]\n",
    "    \n",
    "    # Add the last theta and return result\n",
    "    return result + thetas[10]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've implemented your function. Great job! Now, you have to implement the cost function. We will graph this over iterations to ensure that we're making progress.\n",
    "$$J(θ) = \\frac{1}{n} \\sum_{i=1}^n (f(θ, x_i) - y_i)^2$$\n",
    "\n",
    "Note that while there are `trainingData` and `trainingOutputs` parameters in the function, you can ignore them for now. These will come in handy later, when we'll be testing our validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(thetas, trainingData=trainingData, trainingOutputs=trainingOutputs):\n",
    "    # Type your code here\n",
    "    result = 0\n",
    "    for i in range(len(trainingData)):\n",
    "        result += (f(thetas, trainingData[i]) - trainingOutputs[i, 0])**2\n",
    "    return result/len(trainingData)\n",
    "\n",
    "checkJ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Stuck or completed? Click here to reveal a working example function that you could've written.</summary>\n",
    "\n",
    "```py\n",
    "def J(thetas):\n",
    "    # Loop through each row, adding results together\n",
    "    result = 0\n",
    "    for i in range(len(trainingData)):\n",
    "        # Two arrays go into f, number returned\n",
    "        # Subtract prediction and expected output, square, and add to result\n",
    "        result += (f(thetas, trainingData[i]) - trainingOutputs[i, 0])**2\n",
    "    # Take the average and return\n",
    "    return result/len(trainingData)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it's time to implement the gradients. If you don't know how to take the gradient of the regression equation, then click below to view them.\n",
    "\n",
    "<details>\n",
    "<summary>View Gradients</summary>\n",
    "<details>\n",
    "<summary>\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_1} J(\\theta)$</summary>\n",
    "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{vertical\\_length}})$$\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_2} J(\\theta)$</summary>\n",
    "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{vertical\\_length}}^2)$$\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_3} J(\\theta)$</summary>\n",
    "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{diagonal\\_length}})$$\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_4} J(\\theta)$</summary>\n",
    "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{diagonal\\_length}}^2)$$\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_5} J(\\theta)$</summary>\n",
    "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{cross\\_length}})$$\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_6} J(\\theta)$</summary>\n",
    "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{cross\\_length}}^2)$$\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_7} J(\\theta)$</summary>\n",
    "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{height}})$$\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_8} J(\\theta)$</summary>\n",
    "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{height}}^2)$$\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_9} J(\\theta)$</summary>\n",
    "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{diagonal\\_width}})$$\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_{10}} J(\\theta)$</summary>\n",
    "$$\\frac{2}{n} \\sum_{i=1}^n ((f(θ, x_i) - y_i) * x_{i,\\, \\text{diagonal\\_width}}^2)$$\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_{11}} J(\\theta)$</summary>\n",
    "$$\\frac{2}{n} \\sum_{i=1}^n (f(θ, x_i) - y_i)$$\n",
    "</details>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Implementation Hint</summary>\n",
    "\n",
    "Try to use the fact that all the partial derivatives have $f(θ, x_i) - y_i$ and $\\frac{2}{n}$ in common to \"parallel process\" these.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGradients(thetas):\n",
    "    # Type your code here\n",
    "    n = len(trainingData)\n",
    "    results = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "    for i in range(n):\n",
    "        resultPart = f(thetas, trainingData[i]) - trainingOutputs[i, 0]\n",
    "        for j in range(10):\n",
    "            results[j] += resultPart * trainingData[i, j]\n",
    "        results[10] += resultPart\n",
    "    \n",
    "    for i in range(11):\n",
    "        results[i] = 2 * results[i] / n\n",
    "\n",
    "    return results\n",
    "\n",
    "checkGradients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Stuck or completed? Click here to reveal a working example function that you could've written.</summary>\n",
    "\n",
    "```py\n",
    "def getGradients(thetas):\n",
    "    # Get length of training data\n",
    "    n = len(trainingData)\n",
    "\n",
    "    # Store gradient values\n",
    "    results = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "    # Loop through each training datapoint\n",
    "    for i in range(n):\n",
    "        # Calculate the part which all the gradients have in common\n",
    "        resultPart = f(thetas, trainingData[i]) - trainingOutputs[i, 0]\n",
    "        for j in range(10):\n",
    "            # Multiply the \"uncommon\" part for each and add to the sum\n",
    "            results[j] += resultPart * trainingData[i, j]\n",
    "        results[10] += resultPart\n",
    "    \n",
    "    # Multiply each sum by 2/n to get the gradient for each theta\n",
    "    for i in range(11):\n",
    "        results[i] = 2 * results[i] / n\n",
    "\n",
    "    return results\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's time to implement a complete iteration that takes in $\\theta$ values, and returns new $\\theta$ values after performing a gradient descent step with step size of 0.0000003.\n",
    "\n",
    "If you're wondering why it's such a weird number, it's because that's the value that works good in this scenario, as found by trial and error. To do this yourself, simply print the gradients and see if they're converging to a minimum efficiently.\n",
    "\n",
    "Keep in mind whether you implemented the `getGradients` function to return a python list or a numpy array.\n",
    "\n",
    "This should be relatively easy, so there isn't a check for this part - you can just compare your code with the example code after you're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(thetas):\n",
    "    # Type your code here\n",
    "    gradients = getGradients(thetas)\n",
    "    for i in range(11):\n",
    "        thetas[i] = thetas[i] - 0.0000003*gradients[i]\n",
    "\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Stuck or completed? Click here to reveal a working example function that you could've written.</summary>\n",
    "\n",
    "```py\n",
    "def step(thetas):\n",
    "    # Get the gradients\n",
    "    gradients = getGradients(thetas)\n",
    "\n",
    "    # Loop through and update each theta\n",
    "    for i in range(11):\n",
    "        thetas[i] = thetas[i] - 0.0000003*gradients[i]\n",
    "    \n",
    "    return thetas\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you're done implementing all of the functions, run the next cell to perform 500 iterations, and see how the cost function goes down.\n",
    "\n",
    "Feel free to adjust the number of iterations, stored in the variable `iterations`, and see how the cost changes over time. If you even change it to 5, you should be able to see how the cost dramatically goes down, showing how effective this algorithm is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations to run\n",
    "iterations = 1000\n",
    "\n",
    "# Set any values for theta\n",
    "thetas = np.zeros(11)\n",
    "\n",
    "# Store cost history for plotting later\n",
    "cost_history = [J(thetas)]\n",
    "\n",
    "# Run iterations - perform a step + store cost in history for plotting later\n",
    "for _ in range(iterations):\n",
    "    thetas = step(thetas)\n",
    "    cost_history.append(J(thetas))\n",
    "\n",
    "print(\"Starting cost:\", cost_history[0])\n",
    "print(\"Ending cost:\", cost_history[-1])\n",
    "\n",
    "# Plot cost\n",
    "plt.figure()\n",
    "plt.plot(np.arange(iterations+1), cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait, we're not done! We still need to use out validation data.\n",
    "\n",
    "Remember how we had parameters `trainingData` and `trainingOutputs` in our `J` and `getGradient` functions? Those are optional parameters, with a default value which corresponds to the training data and outputs. However, we can change that by calling the function with `validationData` and `validationOutputs`.\n",
    "\n",
    "Now, using the code above, implement a similar version that measures the cost using the validation data. Remember that you only want to change the dataset for which the cost is measured, not train it for that new dataset. This is because you want to make sure that your model generalizes well.\n",
    "\n",
    "The code above has been copied below for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations to run\n",
    "iterations = 1000\n",
    "\n",
    "# Set any values for theta\n",
    "thetas = np.zeros(11)\n",
    "\n",
    "# Store cost history for plotting later\n",
    "cost_history = [J(thetas)]\n",
    "\n",
    "# Run iterations - perform a step + store cost in history for plotting later\n",
    "for _ in range(iterations):\n",
    "    thetas = step(thetas)\n",
    "    cost_history.append(J(thetas))\n",
    "\n",
    "print(\"Starting cost:\", cost_history[0])\n",
    "print(\"Ending cost:\", cost_history[-1])\n",
    "\n",
    "# Plot cost\n",
    "plt.figure()\n",
    "plt.plot(np.arange(iterations+1), cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Stuck or completed? Click here to reveal a working example function that you could've written.</summary>\n",
    "\n",
    "```py\n",
    "# Number of iterations to run\n",
    "iterations = 1000\n",
    "\n",
    "# Set any values for theta\n",
    "thetas = np.zeros(11)\n",
    "\n",
    "# Store cost history for plotting later\n",
    "cost_history = [J(thetas)]\n",
    "\n",
    "# Run iterations - perform a step + store cost in history for plotting later\n",
    "for _ in range(iterations):\n",
    "    thetas = step(thetas)\n",
    "    # Modified here - pass validation data to the cost function J\n",
    "    cost_history.append(J(thetas, validationData, validationOutputs))\n",
    "\n",
    "print(\"Starting cost:\", cost_history[0])\n",
    "print(\"Ending cost:\", cost_history[-1])\n",
    "\n",
    "# Plot cost\n",
    "plt.figure()\n",
    "plt.plot(np.arange(iterations+1), cost_history)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now you know for sure that your model works. But how well? We can measure that using something called the coefficient of determination, or $R^2$ for short. This provides a measure for how good of a fit your model is, and is calculated by the following formula:\n",
    "$$R^2 = 1-\\frac{\\sum_{i=1}^n (f(\\theta, x_i) - y_i)^2}{\\sum_{i=1}^n (y_{average} - y_i)^2}$$\n",
    "The value of $R^2$ always falls between 0 and 1, where 1 represents a perfect fit. We can use this to judge the accuracy of our model as a percentage.\n",
    "\n",
    "Implement the formula above using the validation data to quantify the accuracy level of our model in the next cell. You should have around a 90% accuracy, but there will be fluctuations due to randomization in the splitting of the original data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value of thetas carry over from previous cell\n",
    "\n",
    "r_squared = 0\n",
    "\n",
    "# Type your code here\n",
    "# The value of r_squared should be a decimal between 0 and 1 after calculations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Value of thetas carry over from previous cell\n",
    "\n",
    "# Get number of rows\n",
    "n = len(validationData)\n",
    "\n",
    "# Get the mean of the output values\n",
    "averageValidationOutput = np.mean(validationOutputs)\n",
    "\n",
    "# Calculate numerator and denominator sum with loop\n",
    "numerator = 0\n",
    "denominator = 0\n",
    "\n",
    "for i in range(n):\n",
    "    numerator += (f(thetas, validationData[i]) - validationOutputs[i, 0])**2\n",
    "    denominator += (averageValidationOutput - validationOutputs[i, 0])**2\n",
    "\n",
    "# Plug into formula\n",
    "r_squared = 1 - numerator/denominator\n",
    "\n",
    "# Display R^2 as percentage\n",
    "print(f\"{'%.2f' % (r_squared*100)}% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Stuck or completed? Click here to reveal a working example function that you could've written.</summary>\n",
    "\n",
    "```py\n",
    "# Value of thetas carry over from previous cell\n",
    "\n",
    "# Get number of rows\n",
    "n = len(validationData)\n",
    "\n",
    "# Get the mean of the output values\n",
    "averageValidationOutput = np.mean(validationOutputs)\n",
    "\n",
    "# Calculate numerator and denominator sum with loop\n",
    "numerator = 0\n",
    "denominator = 0\n",
    "\n",
    "for i in range(n):\n",
    "    numerator += (f(thetas, validationData[i]) - validationOutputs[i, 0])**2\n",
    "    denominator += (averageValidationOutput - validationOutputs[i, 0])**2\n",
    "\n",
    "# Plug into formula\n",
    "r_squared = 1 - numerator/denominator\n",
    "\n",
    "# Display as percentage\n",
    "print(f\"{'%.2f' % (r_squared*100)}% accuracy\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratuations, you've now successfully completed the lab! But, as you can tell, that was a lot of work. We can make these calculations a lot easier and faster by utilizing matrix computations and numpy's arrays to our advantage, which we will learn about in the next article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits\n",
    "* This lab used a modified version of Aung Pyae's fish market dataset from Kaggle. You can find the original dataset [here](https://www.kaggle.com/datasets/aungpyaeap/fish-market).\n",
    "* Formula to calculate $R^2$ accuracy from [Newcastle University, UK](https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/coefficient-of-determination-r-squared.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce9e5fc50149dfb09ca60d84ae2680dbf18b569ea9aa8de02c26aa2ebff8fc36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
