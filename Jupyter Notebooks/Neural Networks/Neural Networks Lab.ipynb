{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADLeotqAMRKR"
      },
      "source": [
        "# Neural Networks Lab\n",
        "## Preprocessing Data\n",
        "For this lab, there isn't any preprocessing! However, importing the data is pretty tricky, as it involves dealing with binaries and libraries and whatnot. I used [this Stack Overflow thread](https://stackoverflow.com/questions/48355140) to help import the data.\n",
        "\n",
        "In the cell below, I've imported the training and validation datasets for you in advance. Simply run it, and continue with the rest of the lab!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tjOLGebktQM"
      },
      "outputs": [],
      "source": [
        "from io import BytesIO\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "# Load and read the data - https://stackoverflow.com/questions/48355140\n",
        "data = np.load(\n",
        "    BytesIO(\n",
        "        requests.get(\n",
        "            'https://github.com/Endothermic-Dragon/Polygence/raw/master/Jupyter%20Notebooks/Neural%20Networks/mnist.npz',\n",
        "            stream = True\n",
        "        ).raw.read()\n",
        "    )\n",
        ")\n",
        "\n",
        "# Assign the data to appropriate variables\n",
        "trainingData = data[\"x_train\"]\n",
        "trainingOutputs = data[\"y_train\"]\n",
        "validationData = data[\"x_test\"]\n",
        "validationOutputs = data[\"y_test\"]\n",
        "\n",
        "# Delete unnecessary variables\n",
        "del BytesIO, requests, data\n",
        "\n",
        "print(\"Successfully loaded dataset!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQpwA_DVSID5"
      },
      "source": [
        "## Visualize the Data\n",
        "\n",
        "Next, let's visualize the data you're working with. All of the training data is in the form of a $28 \\times 28$ array, with each cell holding a pixel value from 1 to 255. The code below displays an $n \\times n$ grid of the training data. Analyze how the code works and update the value of $n$ to visualize a larger sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAR00AIvzsnO"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "n=5\n",
        "fig, axes = plt.subplots(nrows=n, ncols=n, figsize=(n*1.5,n*1.5))\n",
        "\n",
        "for i in range(n**2):\n",
        "    ax = axes[i//n][i%n]\n",
        "    ax.axis(\"off\")\n",
        "    ax.imshow(trainingData[i], cmap=plt.get_cmap('gray'))\n",
        "\n",
        "fig.subplots_adjust(wspace=0.2)\n",
        "plt.show()\n",
        "del n, fig, axes, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2Omr9tMd4wE"
      },
      "source": [
        "Currently, each data point in the training and validation dataset inputs are in a 28 by 28 array. \"Flatten\" these data points so that each input is simply a series of 784 pixel values.\n",
        "\n",
        "You may find the `.reshape(...)` function helpful (documentation [here](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html)), along with the `.shape` property of an array, which returns the length of the dimensions (documentation [here](https://numpy.org/doc/stable/reference/generated/numpy.shape.html)).\n",
        "\n",
        "To more easily understand the `shape` attribute, it might be easier to know how it works on the \"inside\" rather than to visualize it in 2D or 3D. Getting the `shape` attribute returns an array. The first element of the array specifies how many lists/elements are in the overarching array. The second element of the array specifies how many lists/elements are in each list of the overarching array. The third element of the list specifies how many lists/elements are in each list in each list of the overarching array. When visualized in 2D, the first number specifies the number of in the array, which is just the number of rows; the second number specifies the number of elements in each list of the array, which is the number of columns.\n",
        "\n",
        "Note that when reshaping, your goal is to preserve the number of rows as that corresponds to individual data points, but to \"merge\" the 2D arrays in each row into a 1D array (via the use or `.reshape(...)`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sen9If6izueZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoVJqkDtQQIW"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example program that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "# Reshape the training data\n",
        "trainingData = trainingData.reshape(trainingData.shape[0], trainingData.shape[1]*trainingData.shape[2])\n",
        "# Reshape the validation data\n",
        "validationData = validationData.reshape(validationData.shape[0], validationData.shape[1]*validationData.shape[2])\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEoncJIPk8is"
      },
      "source": [
        "The training and validation outputs are simply in the form of an integer between 0 and 9 (inclusive) as of now. However, in order to effectively calculate cost functions and gradients, it's important to have them in the format of an array.\n",
        "\n",
        "Let's take an example. For a row, let's assume that the expected output is 2. Since we are dealing with simulated neurons, we want the element at index `2` to be 1, and all the other 9 possible outputs to be 0. Thus, the corresponding array to replace this output would be `[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]`.\n",
        "\n",
        "Your job is to convert `trainingOutputs` and `validationOutputs` to the required form in the next cell. Note that this process doesn't necessarily have to be efficient, since it's only done once.\n",
        "\n",
        "<details>\n",
        "<summary>What I did</summary>\n",
        "\n",
        "I initialized a new 2D array with the same number of rows and 10 columns filled with zeros using `np.zeros(...)`. After that, I looped through each row in `trainingOutputs`, and updated the index in the newly initialized array with 1's at the appropriate location. Then, I set `trainingOutputs` equal to the new array I created.\n",
        "\n",
        "Finally, I repeated this process for `validationOutputs`.\n",
        "\n",
        "</details>\n",
        "\n",
        "After completing this step, delete any unnecessary variables using `del [first variable name], [second variable name], ...`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS7yuZsdGq7D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K4CQYI9nrhv"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example program that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "# Initialize a new array of 0s\n",
        "newTrainingOutputs = np.zeros((trainingOutputs.shape[0], 10))\n",
        "\n",
        "# Loop through each row of training outputs\n",
        "for i in range(len(trainingOutputs)):\n",
        "    # Set the appropriate index to 1\n",
        "    newTrainingOutputs[i, trainingOutputs[i]] = 1\n",
        "\n",
        "# Replace old array and delete temporary array\n",
        "trainingOutputs = newTrainingOutputs\n",
        "del newTrainingOutputs\n",
        "\n",
        "# Initialize a new array of 0s\n",
        "newValidationOutputs = np.zeros((validationOutputs.shape[0], 10))\n",
        "\n",
        "# Loop through each row of validation outputs\n",
        "for i in range(len(validationOutputs)):\n",
        "    # Set the appropriate index to 1\n",
        "    newValidationOutputs[i, validationOutputs[i]] = 1\n",
        "\n",
        "# Replace old array and delete temporary array\n",
        "validationOutputs = newValidationOutputs\n",
        "del newValidationOutputs\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvbDB2RAe14A"
      },
      "source": [
        "## Training\n",
        "### Quick Note\n",
        "This part of the lab is quite long, despite the decievingly short appearance. If this takes two or three days, don't worry! That's expected. In addition, if you get stuck, there are also individual solutions below (as always)!\n",
        "\n",
        "In addition, you might have to some juggling, as Google Colab has a limited amount of RAM to work with. You can do this by deleting larger arrays or variables that you don't need anymore using `del [first variable name], [second variable name], ...`. Or, if you have python installed, you can download this colab and do the lab locally (no need to worry about RAM).\n",
        "\n",
        "### Neural Network Structure\n",
        "While a neural net's structure is completely up to you to decide, we'll be using 25 nodes for the hidden layer (excluding bias term) and 10 nodes for the output layer.\n",
        "\n",
        "Our images have dimensions of 28 by 28 pixels - in total, that's 784 pixels. Given that we also need a bias term, we can say we have 785 input features.\n",
        "\n",
        "Now, we have the dimensions of our weights by layer, along with our forward propagation procedure:\n",
        "- First to second layer - 785 by 25 array\n",
        "    - Note that currently we have 784 features - we need to add another column at the beginning filled with 1s, which will serve as the bias node.\n",
        "    - We'll by multiplying `trainingData` with this matrix, which should return a matrix with the same numer of rows and 25 columns. Then, we'll apply a sigmoid to each element.\n",
        "- Second to third layer - 26 by 10 array\n",
        "    - After getting the output from the last layer, we'll add another column of 1s for the bias node in the second layer.\n",
        "    - Next, we once again multiply and apply a sigmoid, to obtain a matrix with 10 columns, one for each node in the output layer.\n",
        "\n",
        "Great, now we know exactly what we have to do to go forward! However, this is easier said than done. Let's break the big concept down to individual steps.\n",
        "\n",
        "### Initializing Weights\n",
        "When making a neural network, we first have to initialize the weights at random. But where exactly do we choose this \"random\" from? A good choice comes from the range between $-\\epsilon$ and $\\epsilon$, where $\\epsilon$ comes from the following formula:\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$\\epsilon = \\sqrt{\\frac{6}{\\text{\\# of input nodes} + \\text{\\# of nodes in first hidden layer}}}$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "In the cell below, calculate $\\epsilon$ and use `np.random.uniform(...)` to make 1D arrays of the appropriate sizes, sampled from the appropriate range. You can access the documentation [here](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html).\n",
        "\n",
        "### Basic Functions\n",
        "First of all, write the function for `sigmoid`, keeping in mind that it should be able to handle both numpy arrays and individual numbers. Once again, the formula is:\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$\\sigma(n) = \\frac{1}{1+e^{-n}}$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "Using the explanation above, write the functions `predict`, `J`, and `forwardProp`. You may find [`np.hstack(...)`](https://numpy.org/doc/stable/reference/generated/numpy.hstack.html), [`np.ones(...)`](https://numpy.org/doc/stable/reference/generated/numpy.ones.html), and dot product [`@`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) functions useful.\n",
        "\n",
        "Here are the specifications of their inputs and outputs:\n",
        "- `predict`\n",
        "    - The input is simply the array `trainingData`\n",
        "    - Stack a column of 1's before it via `np.hstack(...)`\n",
        "    - Multiply with first layer of weights\n",
        "    - Apply a sigmoid\n",
        "    - Stack a column of 1's before it again\n",
        "    - Multiply with second layer of weights\n",
        "    - Apply a sigmoid\n",
        "- `J`\n",
        "    - Pass through the `predict` function\n",
        "    - Apply the loss function without the negative (we'll apply that at the end)\n",
        "    - Take the sum of each row and the average of each column (can be done in either order)\n",
        "    - Negate the value\n",
        "    - Apply regularization\n",
        "        - Due to how dot products work in matrices, the topmost row in each array of weights is multiplied to the bias node. Remove this row before squaring, applying `np.sum(...)`, and multiplying by $\\lambda$ (passed into function as `l`). View [this documentation](https://numpy.org/doc/stable/user/basics.indexing.html) for a refresher on how to work with indices.\n",
        "- `forwardProp`\n",
        "    - Perform the calculations as `predict`\n",
        "    - Return a python list of \"outputs\" from each layer (should return a list of 3 numpy arrays)\n",
        "        - Make sure to apply `sigmoid` and add a bias column\n",
        "        - For first layer, since it's just the input data, you don't need to apply a `sigmoid`\n",
        "    - The return values will be used in back propagation when calculating the gradients of the weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCToNPfIeyyk"
      },
      "outputs": [],
      "source": [
        "epsilon = 0\n",
        "thetas1 = []\n",
        "thetas2 = []\n",
        "weights = np.array([thetas1, thetas2], dtype=object)\n",
        "del epsilon, thetas1, thetas2\n",
        "\n",
        "def sigmoid(n):\n",
        "    return\n",
        "\n",
        "def predict(weights, data=trainingData):\n",
        "    return\n",
        "\n",
        "def J(weights, l, data=trainingData, outputs=trainingOutputs):\n",
        "    return\n",
        "\n",
        "def forwardProp(weights, data=trainingData):\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgnKACKhRwYD"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a possible solution to initializing the weights.</summary>\n",
        "\n",
        "```python\n",
        "epsilon = np.sqrt(6/(784+25))\n",
        "thetas1 = np.random.uniform(-epsilon, epsilon, (785, 25))\n",
        "thetas2 = np.random.uniform(-epsilon, epsilon, (26, 10))\n",
        "weights = np.array([thetas1, thetas2], dtype=object)\n",
        "del epsilon, thetas1, thetas2\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a possible solution for the <code>sigmoid</code> function.</summary>\n",
        "\n",
        "```python\n",
        "def sigmoid(n):\n",
        "    return 1/(1+np.e**(-n))\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a possible solution for the <code>predict</code> function.</summary>\n",
        "\n",
        "```python\n",
        "def predict(weights, data=trainingData):\n",
        "    # Apply sigmoid to each node after propagating from first to second layer\n",
        "    layer2_pre = sigmoid(\n",
        "        # Append column of 1s to beginning\n",
        "        np.hstack([\n",
        "            np.ones((data.shape[0], 1)),\n",
        "            data\n",
        "        ])\n",
        "\n",
        "        # Multiply by first set of weights and add up at each node\n",
        "        @ weights[0]\n",
        "    )\n",
        "\n",
        "    # Apply sigmoid to each node after propagating from second to third layer\n",
        "    # Return output after sigmoid\n",
        "    return sigmoid(\n",
        "        # Append column of 1s to beginning\n",
        "        np.hstack([\n",
        "            np.ones((layer2_pre.shape[0], 1)),\n",
        "            layer2_pre\n",
        "        ])\n",
        "\n",
        "        # Multiply by second set of weights and add up at each node\n",
        "        @ weights[1]\n",
        "    )\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a possible solution for the <code>J</code> function.</summary>\n",
        "\n",
        "```python\n",
        "def J(weights, l, data=trainingData, outputs=trainingOutputs):\n",
        "    # Plug into formula\n",
        "    result = -np.sum(np.mean(\n",
        "        outputs*np.log(predict(weights, data))\n",
        "        + (1 - outputs)*np.log(1 - predict(weights, data))\n",
        "    , 0))\n",
        "\n",
        "    # Add regularization terms from first set of weights (excluding bias term)\n",
        "    result += l*np.sum(weights[0][1:]**2)\n",
        "    # Add regularization terms from second set of weights (excluding bias term)\n",
        "    result += l*np.sum(weights[1][1:]**2)\n",
        "\n",
        "    # Return cost\n",
        "    return result\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a possible solution for the <code>forwardProp</code> function.</summary>\n",
        "\n",
        "```python\n",
        "def forwardProp(weights, data=trainingData):\n",
        "    # Add bias term to features and save it\n",
        "    layer1 = np.hstack([\n",
        "        np.ones((data.shape[0], 1)),\n",
        "        data\n",
        "    ])\n",
        "\n",
        "    # Propagate one layer and apply sigmoid\n",
        "    layer2_pre = sigmoid(layer1 @ weights[0])\n",
        "\n",
        "    # Add bias term to second layer and save it\n",
        "    layer2 = np.hstack([\n",
        "        np.ones((layer2_pre.shape[0], 1)),\n",
        "        layer2_pre\n",
        "    ])\n",
        "\n",
        "    # Propagate to output layer\n",
        "    layer3 = sigmoid(layer2 @ weights[1])\n",
        "\n",
        "    # Return outputs of each layer\n",
        "    return [layer1, layer2, layer3]\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfONtiYlMa1x"
      },
      "source": [
        "### Final Boss - Backpropagation\n",
        "This part of the lab takes the most time. If you're not prepared to spend, at the very least, half an hour on understanding this, then I'd recommend simply copying and pasting the code from the solution below.\n",
        "\n",
        "That being said, this is the most fun part to understand. After all, who doesn't want to be able to brag that they know and are able to apply advanced PhD level material from the late 1900s? While it may take a bit of time, the euphoria felt when your code works is immeasurable.\n",
        "\n",
        "Before we begin, let's recap the algorithm, which is repeated for each row of data:\n",
        "- Calculate the error for the last layer. Assign this to `delta2`.\n",
        "- Multiply each error by the weight at each connection, add up products at the second layer. Undo the sigmoid. Assign this to `delta1`.\n",
        "- No need to calculate error for the first layer\n",
        "- For each connecting weight, multiply the output value of the previous layer's connected node to the error value of the next layer's connected node\n",
        "- Repeat for all weights and for all rows in the training data\n",
        "\n",
        "Now, since we're trying to optimize our RAM, here's an outline of how to do this algorithm successfully and efficiently:\n",
        "- Get forward propagation results\n",
        "- Delete `data` as you don't need it anymore\n",
        "    - You also have a similar copy from your forward propagation outputs\n",
        "- Calculate `delta2`, and then delete any variables you may have created to calculate it\n",
        "- Delete `outputs` as you don't need it anymore\n",
        "- Calculate the derivative of the second array of weights, assign it to `deriv_weights_1`\n",
        "    - Account for the regularization term\n",
        "        - Account for the fact that first row isn't regularized\n",
        "- Calculate `delta1`, and then delete any variables you may have created to calculate it\n",
        "- Delete `delta2` as you don't need it anymore\n",
        "- Calculate the derivative of the first layer of weights, assign it to `deriv_weights_0`\n",
        "    - Calculate the derivative from the unregularized cost function\n",
        "    - Delete `forwardPropData` and `delta1` as you don't need them anymore\n",
        "    - Account for the regularization term\n",
        "        - Account for the fact that first row isn't regularized\n",
        "    - Put `deriv_weights_0` and `deriv_weights_1` in a ragged numpy array\n",
        "\n",
        "In addition to this procedure, you may need a few extra pieces of information.\n",
        "- Calculating `delta2`\n",
        "    - This is actually quite similar to how the gradients are calculated logistic regression lab. First, you find the derivative of the cost function with respect to each node in the prediction function output and undo the sigmoid for each node. Note how you do NOT take the sum or mean. Here's the formula, which you repeat for all 60,000 rows (i) and 10 columns (j):\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$\\left(\\frac{1-y_{i,j}}{1-\\sigma(\\text{node sum}_{i,j})} -\\frac{y_{i,j}}{\\sigma(\\text{node sum}_{i,j})} \\right) \\cdot \\sigma(\\text{node sum}_{i,j})(1 - \\sigma(\\text{node sum}_{i,j}))$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "    - Note that we don't need the node sum to calculate this, as:\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$\\sigma(\\text{node sum}_{i,j}) = \\text{node output}_{i,j}$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "- Calculating `delta1`\n",
        "    - Each row in `weights[1]` represents all the connections from one node in the previous layer to all the nodes in the next layer. Consequently, in its transposition, each column represents all the connections from all the nodes in the next layer to a single node in the previous layer.\n",
        "    - Performing a dot multiplication between `delta2` and the transposition of `weights[1]` not only multiplies each error to the right weight, but also adds them up. You're almost done with the calculation for `delta1` - you just have to undo the sigmoid! You can do this by multiplying by:\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$\\text{second layer node output}_{i,j} \\cdot (1 - \\text{second layer node output}_{i,j})$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "    - After this step, remember to strip the first column because the errors for the bias term won't help us - not to mention that the entire column is 0 anyways, as a result of undoing the sigmoid.\n",
        "- Calculating `deriv_weights_0` and `deriv_weights_1` WITHOUT regularization\n",
        "    - At this point, you're trying to somehow combine two large 2D arrays, but you don't know how. To simplify this case, let's just take the first row from each. You know that the output should be of the same shape as the corresponding layer of weights. In addition, you know that the number of elements in the row of errors corresponds to the number of columns in the corresponding layer's weight. Similarly, the number of elements in the row of the previous layer's outputs corresponds to the number of rows in the corresponding layer's weight.\n",
        "    - Now, you can leverage the power of [broadcasting (see figure 4)](https://numpy.org/doc/stable/user/basics.broadcasting.html)! Specifically, you can make the row of the previous layer's outputs into a vertical 2D array and the row of errors into a horizontal 2D array, and multiply them.\n",
        "    - You need to repeat this concept, but on a 3D scale. How do you do this, you ask? Well, you can turn each 2D array into a 3D array, using `np.newaxis` (see [Stack Overflow](https://stackoverflow.com/questions/29241056) and [numpy documentation](https://numpy.org/doc/stable/reference/constants.html#numpy.newaxis))! Before doing anything, take note that you want to conserve the number of rows.\n",
        "    - In a 2D array `a`, `a[:, np.newaxis, :]` will add brackets one layer in. Specifically, each row will be double-braced with brackets.\n",
        "        - Essentially, each row will have a horizontal 2D array in it.\n",
        "    - In a 2D array `a`, `a[:, :, np.newaxis]` will add brackets two layers in. Specifically, each element will have a brace around it.\n",
        "        - Essentially, each row will have a vertical 2D array in it.\n",
        "    - Ok, now you have a huge 3D array! Time to collapse it. Take the mean of all the 2D matrices in the 60,000 row array. Remember to set the axis parameter properly.\n",
        "- Adding regularization to `deriv_weights_0` and `deriv_weights_1`\n",
        "    - The derivative of the regularization term is $2\\lambda\\theta$. For each layer, calculate this value for the entire array of $\\theta$s (remember that `l` represents $\\lambda$).\n",
        "    - Before adding this to `deriv_weights_0` or `deriv_weights_1`, remember that the bias term is supposed to be excluded. Accordingly, set the first row of the array of weights to 0. [Here's a shortcut you may find helpful](https://stackoverflow.com/questions/17482955).\n",
        "    - Add it to the nonregularized weight derivatives!\n",
        "- To transpose an array `a`, simply use `a.T`.\n",
        "- To create a ragged numpy array, use `np.array(..., dtype=object)`. This is the same way the ragged array for `weights` is created.\n",
        "\n",
        "Whew! That was a LOT. This is one of the two main reasons people use libraries to do this for them. The other reason is that neural networks are very expensive to train, both in RAM and in time.\n",
        "\n",
        "Good luck on the implementation, and if you need any help, know that you can always refer to the solution snippets below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpSP6dFVxqya"
      },
      "outputs": [],
      "source": [
        "def backProp(weights, l, data=trainingData, outputs=trainingOutputs):\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6EJxYCJcUpu"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a possible solution for calculating <code>delta2</code>.</summary>\n",
        "\n",
        "```python\n",
        "# Get output from forward propagation\n",
        "forwardPropData = forwardProp(weights, data)\n",
        "# Isolate last layer in seperate variable for readability and aesthetics\n",
        "preds = forwardPropData[2]\n",
        "# Calculate the errors on the last layer\n",
        "delta2 = ((1-outputs)/(1-preds) - outputs/preds)*preds*(1-preds)\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a possible solution for the <code>backProp</code> function.</summary>\n",
        "\n",
        "```python\n",
        "def backProp(weights, l, data=trainingData, outputs=trainingOutputs):\n",
        "    # Get output from forward propagation\n",
        "    forwardPropData = forwardProp(weights, data)\n",
        "    # Delete unnecessary variables\n",
        "    del data\n",
        "\n",
        "    # Isolate last layer in seperate variable for readability and aesthetics\n",
        "    preds = forwardPropData[2]\n",
        "    # Calculate the errors on the last layer\n",
        "    delta2 = ((1-outputs)/(1-preds) - outputs/preds)*preds*(1-preds)\n",
        "    # Delete unnecessary variables\n",
        "    del outputs, preds\n",
        "\n",
        "    # Get the derivative of the second set of weights\n",
        "    # Unregularized\n",
        "    deriv_weights_1 = np.mean(\n",
        "        delta2[:,np.newaxis,:]*forwardPropData[1][:,:,np.newaxis]\n",
        "    , 0)\n",
        "    # Add regularization\n",
        "    reg_term = 2*l*weights[1]\n",
        "    # Zero out bias term regularizations\n",
        "    reg_term[0] = 0\n",
        "    # Put it together\n",
        "    deriv_weights_1 += reg_term\n",
        "    # Delete unnecessary variables\n",
        "    del reg_term\n",
        "\n",
        "    # Calculate the errors on the hidden second layer + remove first column\n",
        "    delta1 = (\n",
        "        delta2 @ weights[1].T * forwardPropData[1]*(1-forwardPropData[1])\n",
        "    )[:,1:]\n",
        "    # Delete unnecessary variables\n",
        "    del delta2\n",
        "\n",
        "    # Get the derivative of the first set of weights\n",
        "    # Unregularized\n",
        "    deriv_weights_0 = np.mean(\n",
        "        delta1[:,np.newaxis,:]*forwardPropData[0][:,:,np.newaxis]\n",
        "    , 0)\n",
        "    # Delete unnecessary variables\n",
        "    del forwardPropData, delta1\n",
        "    # Add regularization\n",
        "    reg_term = 2*l*weights[0]\n",
        "    # Zero out bias term regularizations\n",
        "    reg_term[0] = 0\n",
        "    # Put it together\n",
        "    deriv_weights_0 += reg_term\n",
        "\n",
        "    # Group derivatives into the same format as the weights and return\n",
        "    return np.array([deriv_weights_0, deriv_weights_1], dtype=object)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3dQ1CGLPfQO"
      },
      "source": [
        "### Learning Rate\n",
        "Taking inspiration from [this article](https://towardsdatascience.com/https-medium-com-super-convergence-very-fast-training-of-neural-networks-using-large-learning-rates-decb689b9eb0) and the concept of superconvergence, I've implemented a variable learning rate below. Specifically, the inpiration comes from the typical trend seen when training. Initially, there are steep gradients that you can traverse quickly, but then you reach flat valleys. In those areas, a higher learning rate helps speed things up. Then, as you approach a local minimum, you slow down again to converge carefully.\n",
        "\n",
        "After this intuition, I tinkered around with the learning rate for some more time, along with the regularization coefficient to pick good settings for both. Speaking of the regularization coefficient...\n",
        "\n",
        "### Regularization\n",
        "I also did quite a bit of testing with coefficients! I used the variable learning rate from the generator function below for all of these tests, and ran them for 100 iterations. Here's a summary of my results:\n",
        "- Coefficient of `0`: average of 81.73% accuracy\n",
        "    - 80.68%\n",
        "    - 78.03%\n",
        "    - 84.04%\n",
        "    - 84.17%\n",
        "- Coefficient of `0.0005`: average of 82.245% accuracy\n",
        "    - 82.63%\n",
        "    - 79.03%\n",
        "    - 85.71%\n",
        "    - 81.61%\n",
        "- Coefficient of `0.001`: average of 84.1525% accuracy\n",
        "    - 83.72%\n",
        "    - 84.29%\n",
        "    - 84.47%\n",
        "    - 84.13%\n",
        "- Coefficient of `0.0015`: average of 83.2325% accuracy\n",
        "    - 82.96%\n",
        "    - 86.14%\n",
        "    - 81.74%\n",
        "    - 82.09%\n",
        "- Coefficient of `0.002`: average of 83.7975% accuracy\n",
        "    - 84.67%\n",
        "    - 84.09%\n",
        "    - 81.57%\n",
        "    - 84.86%\n",
        "- Coefficient of `0.0025`: average of 83.405% accuracy\n",
        "    - 81.08%\n",
        "    - 85.73%\n",
        "- Coefficient of `0.003`: average of 82.005% accuracy\n",
        "    - 81.81%\n",
        "    - 82.2%\n",
        "\n",
        "To me, both `0.001` and `0.0015` seemed like good candidates, so I ran both of them for 300 iterations. I got 87.52% accuracy for `0.001`, and 87.34% accuracy for `0.0015`.\n",
        "\n",
        "To conclude the results of this testing, I recommend using `0.001` as your learning coefficient, and the variable learning rate generator below to speed up your training process and achieve a high accuracy.\n",
        "\n",
        "Update the code below to use the appropriate value for $\\lambda$ (`l`), and feel free to adjust the number of iterations run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAAn3PbOYKYX"
      },
      "outputs": [],
      "source": [
        "def stepLearner():\n",
        "    yield from np.linspace(0.2, 0.5, 20)\n",
        "    yield from np.linspace(0.4, 0.25, 15)\n",
        "    yield from np.linspace(0.25, 0.2, 25)\n",
        "    for _ in range(6):\n",
        "        yield from np.linspace(0.2, 0.15, 10)\n",
        "    for _ in range(5):\n",
        "        yield from np.linspace(0.15, 0.1, 10)\n",
        "    for _ in range(5):\n",
        "        yield from np.linspace(0.1, 0.07, 10)\n",
        "    while True:\n",
        "        yield 0.05\n",
        "\n",
        "# Update value of lambda\n",
        "l = 0\n",
        "learningRate = stepLearner()\n",
        "iterations = 100\n",
        "cost_history = [J(weights, l)]\n",
        "\n",
        "for i in range(iterations):\n",
        "    weightGradients = backProp(weights, l)\n",
        "    weights = weights - next(learningRate) * weightGradients\n",
        "    del weightGradients\n",
        "    cost_history.append(J(weights, l))\n",
        "    print(f\"Iteration {i+1}, Cost {cost_history[-1]}\")\n",
        "\n",
        "print(\"Initial cost:\", cost_history[0])\n",
        "print(\"Final cost:\", cost_history[-1])\n",
        "plt.plot(np.arange(iterations+1), cost_history)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHABbRaZB76L"
      },
      "source": [
        "## Judging Model\n",
        "After all of that training, it's time to judge how good our model is.\n",
        "\n",
        "First, we need to check overfitting. Calculate the cost of the model on the validation dataset. Make sure to pass the value of $\\lambda$. Check if it's comparable with the cost from our training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-wjOVydezrO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpZKVB56fAGz"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example program that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "print(\"Validation cost:\", J(weights, l, validationData, validationOutputs))\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f55aFVipFkgA"
      },
      "source": [
        "Now, to find the accuracy, calculate what percent of your model's predictions on the validation dataset is correct. You may find the `np.argmax(...)` function helpful. Also take note that you can call `np.sum(...)` on a boolean list, which will treat `True` values as 1 and `False` values as 0.\n",
        "\n",
        "Your accuracy should be around 84% for 100 iterations. For more iterations, you could achieve accuracy values closer to 90%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fvSW4ROHV0E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA9riBTJfCrH"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example program that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "comparison = np.argmax(predict(weights, validationData), 1) == np.argmax(validationOutputs, 1)\n",
        "print(f\"Model accuracy: {(np.sum(comparison) / comparison.size * 100).round(2)}%\")\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpWQ1ooQFvgA"
      },
      "source": [
        "## Credits\n",
        "- This lab used the MNIST handwritten digits dataset. You can find the original dataset [here](https://yann.lecun.com/exdb/mnist/)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "ce9e5fc50149dfb09ca60d84ae2680dbf18b569ea9aa8de02c26aa2ebff8fc36"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
