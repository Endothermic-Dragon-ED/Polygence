{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HudX9BBE9g25"
      },
      "source": [
        "# Logistic Regression Lab\n",
        "## Preprocessing Data\n",
        "The dataset you're going to work with is already in pretty good shape in itself, so it shouldn't require a lot of preprocessing. But before any of that, we have to import the data first!\n",
        "### Import The Data\n",
        "You know the deal! Import the numpy and pandas libraries as `np` and pandas as `pd`, import the data from [this github link](https://raw.githubusercontent.com/Endothermic-Dragon/Polygence/master/Jupyter%20Notebooks/Logistic%20Regression/Heart%20Attack%20Possibility.csv) and save it as `dataTable`, and display the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyHepVnvGfhn"
      },
      "outputs": [],
      "source": [
        "# Import numpy as np, and pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Get data from CSV file on GitHub\n",
        "dataTable = \n",
        "\n",
        "# Display data table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGlIwAuoJNgs"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example program that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "# Import numpy as np, and pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Get data from CSV file on GitHub\n",
        "dataTable = pd.read_csv(\"https://raw.githubusercontent.com/Endothermic-Dragon/Polygence/master/Jupyter%20Notebooks/Logistic%20Regression/Heart%20Attack%20Possibility.csv\")\n",
        "\n",
        "# Display data table\n",
        "dataTable\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc1Qv78Bk7yO"
      },
      "source": [
        "Our overall goal is to predict the value of `target`, which is the diagnosis of having a heart disease.\n",
        "\n",
        "If you want to know what each column actually represents, see:\n",
        "- [The Kaggle dataset I used for this lab](https://www.kaggle.com/datasets/nareshbhat/health-care-data-set-on-heart-attack-possibility)\n",
        "- [The original dataset that the Kaggle dataset borrowed from](https://archive.ics.uci.edu/ml/datasets/Heart+Disease)\n",
        "    - Specifically, look under the 14 attributes listed under the \"Attribute Information\" section\n",
        "\n",
        "Otherwise, continue with the lab!\n",
        "\n",
        "## Analyzing The Data\n",
        "First, let's analyze relations.\n",
        "\n",
        "We'll be using `seaborn` to plot the data. `sex`, `cp`, `fbs`, `restecg`, `exang`, `slope`, `ca`, and `thal` all are discrete values, so plotting them won't do us any favors. Create a copy of the dataset called `dataTableCopy` without those columns.\n",
        "\n",
        "<details>\n",
        "<summary>Hint</summary>\n",
        "\n",
        "You can do this using `dataTable.drop(...)`. The documentation for the `drop` function can be accessed [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html). Make sure to specify both the column names and the axis (rows corresponds to 0, columns corresponds to 1).\n",
        "\n",
        "</details>\n",
        "\n",
        "After creating `dataTableCopy`, graph that using `sns.pairplot(...)`. Set the `hue` parameter to `\"target\"`. This will generate a set of graphs with colored points, where the color is determined by the `target` value. This will help you see any simple relations right away (if there are any).\n",
        "\n",
        "- If you're able to seperate them by a line, then you want to make sure to keep a copy of those two features in degree `1` before classification\n",
        "- If you're able to seperate them by a circle, then you want to make sure to keep a copy of those two features in degree `1` and `2` before classification\n",
        "\n",
        "After that, graph the correlation matrix between the columns of `datatableCopy`.\n",
        "\n",
        "<details>\n",
        "<summary>Hint</summary>\n",
        "\n",
        "You can use `.corr()` on `dataTable` to calculate the correlation matrix, and then use `.round(n)` to round each value in the matrix to `n` digits.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeVJNzJxh5LY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create and show plot\n",
        "dataTableCopy = \n",
        "\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,15))\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr = \n",
        "\n",
        "# Display correlation matrix\n",
        "sns.heatmap(corr, annot=True, ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62PU0W8IJUbE"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example program that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create and show plot\n",
        "dataTableCopy = dataTable.drop([\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"ca\", \"thal\"], axis=1)\n",
        "sns.pairplot(dataTableCopy, hue=\"target\")\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,15))\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr = dataTable.corr().round(2)\n",
        "\n",
        "# Display correlation matrix\n",
        "sns.heatmap(corr, annot=True, ax=ax)\n",
        "plt.show()\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvY6iMdihR7U"
      },
      "source": [
        "Deduce any relations to keep an eye on or to modify, if there are any. You will get a chance to make your own modifications to the features very soon.\n",
        "\n",
        "### Modify The Data\n",
        "Make any modifications to the data that you need below. We will apply a Z Score to this in the next step, so don't worry about it for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MeCr_lEc27J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2lB7hm-rDBJ"
      },
      "source": [
        "<details>\n",
        "<summary>What I did</summary>\n",
        "\n",
        "I didn't do anything! As mentioned before, linear regression doesn't require too much manipulation of the data. Also, I found that not adding squared terms actually results in a slightly higher accuracy üòù.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl91WSiLhxBY"
      },
      "source": [
        "Apply a Z Score to the `age`, `trestbps`, `chol`, and `thalach` columns as they are not binary values and have a large range (they could potentially slow down the gradient descent process). Also apply a Z Score to any columns you may have added in the previous cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ijxjG5whxS0"
      },
      "outputs": [],
      "source": [
        "# Complete the function to calculate the Z Score\n",
        "def zScore(columnData):\n",
        "    return\n",
        "\n",
        "# Implementations may vary, but the basis is set up for you\n",
        "for i in [\"age\", \"trestbps\", \"chol\", \"thalach\"]:\n",
        "    dataTable[i] = zScore(dataTable[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pvzKhWvLEDU"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a possible solution for the <code>zScore</code> function.</summary>\n",
        "\n",
        "```python\n",
        "def zScore(columnData):\n",
        "    return (columnData - np.mean(columnData)) / np.std(columnData)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxoh3LY_odw-"
      },
      "source": [
        "## Get Ready to Train\n",
        "Shuffle the data in the cell below. Use `dataTable.sample(...)`, whose documentation you can find [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html).\n",
        "- Make sure to specify the `frac` parameter and not the default `n` parameter.\n",
        "- You can optionally also set a `random_state` parameter to ensure consistency when randomizing - that way, you get the same results every time you rerun all the cells.\n",
        "- Make sure to set `dataTable` equal to the output.\n",
        "\n",
        "After sampling, apply `dataTable.reset_index(...)` with parameter `drop` set as true. Make sure to set `dataTable` equal to the output.\n",
        "\n",
        "Alternatively, you can do both of these steps as a one-liner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gV2ivf6KymU"
      },
      "outputs": [],
      "source": [
        "dataTable = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzAY4RCNLcWx"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example program that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "dataTable = dataTable.sample(frac=1, random_state=314).reset_index(drop=True)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGM_iYtGoxqZ"
      },
      "source": [
        "Next, we have to split the data into a training set and a validation set. Note that the code for this section has been written for you in this lab. This is because it's trivial and somewhat of a menial task.\n",
        "\n",
        "For now, simply modify `validaitonSetSize`. Note that we aren't working with a huge dataset (only ~300 data points), so I'd recommend choosing a value near 50 or 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZJwKDZOUZla"
      },
      "outputs": [],
      "source": [
        "# Choose the number of data points in the validation set\n",
        "validaitonSetSize = \n",
        "\n",
        "splitPoint = dataTable.shape[0]-validaitonSetSize\n",
        "del validaitonSetSize\n",
        "\n",
        "trainingData = dataTable.drop(labels=\"target\", axis=1).to_numpy()[:splitPoint]\n",
        "trainingData = np.hstack([\n",
        "                          np.ones((splitPoint, 1)),\n",
        "                          trainingData\n",
        "                         ])\n",
        "trainingOutputs = dataTable[[\"target\"]].to_numpy()[:splitPoint]\n",
        "\n",
        "validationData = dataTable.drop(labels=\"target\", axis=1).to_numpy()[splitPoint:]\n",
        "validationData = np.hstack([\n",
        "                          np.ones((len(dataTable) - splitPoint, 1)),\n",
        "                          validationData\n",
        "                           ])\n",
        "validationOutputs = dataTable[[\"target\"]].to_numpy()[splitPoint:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULH9bz5Qph59"
      },
      "source": [
        "## Training\n",
        "Finally, we're here!\n",
        "\n",
        "The framework of the code has been set up for you. First, begin with the `sigmoid` function. Note that this function should be able to handle both individual numbers and numpy arrays. Here's the formula, to jog your memory:\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "This function should be a short one-liner.\n",
        "\n",
        "Next, fill in the prediction function `f`. This should also be a very short one-liner. Remember that the dot product of two arrays can be taken using `@`. Also, don't forget the `sigmoid`!\n",
        "\n",
        "After that, fill in the cost function `J`. Note that you can use `np.mean` instead of using `np.sum` and then dividing by the number of data points. You can set the regularization coefficient, $\\lambda$, as 0.01. Here's the formula to calculate it, with $n$ as the number of rows and $m$ as the number of features (including the bias term):\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$J(\\theta) = - \\frac{1}{n} \\cdot \\sum_{i=1}^n y_i\\log(f(\\theta, x_i)) + (1 - y_i)\\log(1 - f(\\theta, x_i)) + \\lambda \\sum_{i=2}^m \\theta_i^2$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "Note that the regularization of the weights starts from $\\theta_2$ - you can account for this fact by [leveraging array indices](https://numpy.org/doc/stable/user/basics.indexing.html) when manipulating `thetas`. Also take note that `log` represents a natural log (mathematically denoted as ln) and not a base-10 log in the context of computer science and machine learning. This fact is also reflected by the `np.log(...)` function in the numpy library.\n",
        "\n",
        "Finally, we reach the final boss - the function to take the gradients. Once again, if you know calculus, I would recommend figuring this part out by yourself. If you don't, simply expand the explanation below.\n",
        "\n",
        "<details>\n",
        "<summary>Calculating the Gradients</summary>\n",
        "\n",
        "First, let's \"unwrap\" our cost function by ignoring the fact that we're taking the mean and negating the equation. Now, we're left with two terms:\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$y_i\\log(f(\\theta, x_i))$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$(1 - y_i)\\log(1 - f(\\theta, x_i))$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "Before taking the derivative, we also have to plug in the formula for `f`:\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$f(\\theta, x_i) = \\sigma(x_i \\theta)$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "Take note that $x_i$ is a horizontal matrix and $\\theta$ is a vertical matrix, but the output of the function is a numerical value.\n",
        "\n",
        "The derivative of the first term (once again, assuming `log` is the natural log due to the context of computer science) is:\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$\\frac{y_i}{\\sigma(x_i \\theta)} \\cdot \\sigma(x_i \\theta)(1 - \\sigma(x_i \\theta)) \\cdot x_i$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "Note that due to the $x_i$ term at the end, the result will be a matrix.\n",
        "\n",
        "Similarly, the derivative of the second term is:\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$-\\left(\\frac{1-y_i}{1-\\sigma(x_i \\theta)} \\cdot \\sigma(x_i \\theta)(1 - \\sigma(x_i \\theta)) \\cdot x_i \\right)$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "You calculate these two values for each row in the dataset, add both the values up. At this point, you should have a 2D array, where the number of rows is the number of data points, and the number of columns is the number of $\\theta$'s.\n",
        "\n",
        "Note that you can make this computation faster by taking out common terms when adding the two terms. Specifically, you can factor out $\\sigma(x_i \\theta)(1 - \\sigma(x_i \\theta)) \\cdot x_i$ from the two terms. This makes the computer do less individual computations.\n",
        "\n",
        "The next step is to undo our \"unwrapping\". We calculate the average across all rows (aka find the average value in each column), and take the negative of the resulting matrix. At this point, you should be left with a horizontal 1D array. Turn this into a vertical 2D array, as this is the format the $\\theta$'s are in. After that, add the derivative of the regularization terms ($2\\lambda\\theta$) to each array \"cell\", except the bias term. Since the other array is already in the necessary format, you can do this efficiently by multiply `thetas` by $2\\lambda$, zeroing out the bias term, add it to the other array, and return it.\n",
        "\n",
        "To see how to zero out a row or column efficiently, see [this link from Stack Overflow](https://stackoverflow.com/questions/17482955).\n",
        "\n",
        "Another optimization you can make to this gradient-computing process is to bring the outermost negative or the non-regularized cost function all the way on the \"inside\". Before this optimization, you would be adding a positive derivative (first term mentioned above) and a negative derivative (second term mentioned above). Essentially, this is the same as subtracting the second term from the first. However, bringing this negative inside, you could do the same thing in the opposite order. In addition, subtracting the first term from the second doesn't take any more computational steps, and reduces one operation for the computer to do (more acute observation will show that it reduces *more* than one operation, since the negative is performed on each element of an array).\n",
        "\n",
        "Yet another third optimization to perform is to simply precompute the predictions from `f` and saving it in a variable. This is because without precomputation, you're essentially calculating the same thing 4 to 6 times (depending on your implementation). Precalculating this array will simply store the outputs in the memory and fetch it when needed, reducing the number of operations performed. Note that you can also do this for the cost function, but it won't have as much of a benefit as here.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up_Ri5iS8BaA"
      },
      "outputs": [],
      "source": [
        "# Initialize thetas\n",
        "thetas = np.zeros([\n",
        "                    trainingData.shape[1],\n",
        "                    1\n",
        "                  ])\n",
        "\n",
        "def sigmoid(n):\n",
        "    return\n",
        "\n",
        "def f(thetas, data=trainingData):\n",
        "    return\n",
        "\n",
        "# Make sure to use the value of lambda\n",
        "def J(thetas, l, data=trainingData, outputs=trainingOutputs):\n",
        "    return\n",
        "\n",
        "# Make sure to use the value of lambda\n",
        "def getGradients(thetas, l, data=trainingData, outputs=trainingOutputs):\n",
        "    return\n",
        "\n",
        "# Set the value of lambda\n",
        "l = 0\n",
        "iterations = 1000\n",
        "cost_history = []\n",
        "cost_history.append(J(thetas, l))\n",
        "\n",
        "# Train!\n",
        "for i in range(iterations):\n",
        "    thetas = thetas - 0.25 * getGradients(thetas, l)\n",
        "    cost_history.append(J(thetas, l))\n",
        "\n",
        "# Print results\n",
        "print(\"Initial cost:\", cost_history[0])\n",
        "print(\"Final cost:\", cost_history[-1])\n",
        "plt.plot(np.arange(iterations+1), cost_history)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGgawtA98aqT"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a possible solution for the <code>sigmoid</code> function.</summary>\n",
        "\n",
        "```python\n",
        "def sigmoid(n):\n",
        "    return 1/(1 + np.e**(-n))\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a possible solution for the <code>f</code> function.</summary>\n",
        "\n",
        "```python\n",
        "def f(thetas, data=trainingData):\n",
        "    return sigmoid(data @ thetas)\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a possible solution for the <code>J</code> function.</summary>\n",
        "\n",
        "```python\n",
        "def J(thetas, l, data=trainingData, outputs=trainingOutputs):\n",
        "    # Precompute the predictions\n",
        "    predictions = f(thetas, data)\n",
        "    return -np.mean(\n",
        "        # First term inside summation\n",
        "        outputs * np.log(predictions)\n",
        "        # Second term inside summation\n",
        "        + (1-outputs) * np.log(1 - predictions)\n",
        "    )\n",
        "    # Regularization term\n",
        "    + l*np.sum(thetas[1:]**2)\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a possible solution for the <code>getGradients</code> function.</summary>\n",
        "\n",
        "```python\n",
        "def getGradients(thetas, l, data=trainingData, outputs=trainingOutputs):\n",
        "    # Precompute the predictions\n",
        "    predictions = f(thetas, data)\n",
        "    results = np.mean(\n",
        "        # First term's derivative and second terms derivative\n",
        "        # Note the order is flipped because of a possible optimization discussed in the instructions\n",
        "        # Consequently, the \"np.mean\" above doesn't have a negative before it\n",
        "        ((1-outputs) / (1 - predictions) - outputs / predictions)\n",
        "            # Factored out common terms (another optimization)\n",
        "            * predictions * (1-predictions)\n",
        "            * data\n",
        "    , 0)\n",
        "\n",
        "    # Turn 1D array into vertical 2D array\n",
        "    results = np.array([results]).T\n",
        "\n",
        "    # Regularization term\n",
        "    reg_term = 2*l*thetas\n",
        "    reg_term[0] = 0\n",
        "    results += 2*l*reg_term\n",
        "\n",
        "    return results\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuYv4cUL_6x7"
      },
      "source": [
        "Use the next cell to graph the progress of the the model in accordance to the validation data. Make sure to pass the value of $\\lambda$ (`l`). You can use the previous cell as a reference when writing code for this cell, but bear in mind that you don't have to redefine the same functions (namely, `sigmoid`, `f`, `J`, and `getGradients`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6ktcJsJgWnH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2PS-QD6L6vY"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example program that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "# Initialize thetas\n",
        "thetas = np.zeros([\n",
        "                   trainingData.shape[1],\n",
        "                   1\n",
        "                  ])\n",
        "\n",
        "# Set the value of lambda\n",
        "l = 0.01\n",
        "iterations = 1000\n",
        "cost_history = []\n",
        "\n",
        "# Make sure to calculate cost on the validation dataset\n",
        "cost_history.append(J(thetas, l, validationData, validationOutputs))\n",
        "\n",
        "# Train!\n",
        "for i in range(iterations):\n",
        "    # Train using the training dataset\n",
        "    thetas = thetas - 0.25 * getGradients(thetas, l)\n",
        "    # Make sure to calculate cost on the validation dataset\n",
        "    cost_history.append(J(thetas, l, validationData, validationOutputs))\n",
        "\n",
        "# Print results\n",
        "print(\"Initial cost:\", cost_history[0])\n",
        "print(\"Final cost:\", cost_history[-1])\n",
        "plt.plot(np.arange(iterations+1), cost_history)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFDTKt5fvTcn"
      },
      "source": [
        "## Judging Accuracy\n",
        "First, calculate the accuracy of your model by calculating how often its prediction is correct. You should get a value slightly higher than 80%.\n",
        "\n",
        "Note that you can perform element-wise comparisons with `>`, `<`, `>=`, `<=`, and `==` among arrays of the same dimensions or individual values. You can also use `a.astype(int)` to convert any array `a` from an array of booleans (`True` and `False`) to an array of integers (`1` and `0`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikBrOS3ZhOLf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOfinnspMXHc"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example program that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "a = (\n",
        "        (f(thetas, validationData) > 0.5) == validationOutputs\n",
        "    ).astype(int)\n",
        "print(f\"{np.round(np.mean(a) * 100, 2)}%\")\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcP5S78k6Ids"
      },
      "source": [
        "In the exercise above, I got an accuracy around 84%. After I repeated the same training process with the regularization coefficient set to 0 and checked the accuracy, I got a value of 82%. While not a huge boost (in this specific case), it demonstrates the importance of regularization, especially when considering how well a model generalizes to unseen data. You can repeat this exact circumstance by setting the validation dataset size to 100, and performing the shuffling with the same seed I used.\n",
        "\n",
        "If you're wondering how to pick the regularization term, well, there's no hard and fast rule - it's more trial and error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HdLFvlWvsuO"
      },
      "source": [
        "Next, calculate the F1 Score (and its associated parameters). The first step, converting predictions and true values into a 1D array of booleans, has already been done for you.\n",
        "\n",
        "Note that `tp` stands for true positives, `fp` for false positives, and so on. Using numpy's `&` (and) operator and `~` (not) operator should suffice for calculating these values. Do ***NOT*** use python's version of these operators, as they will not work element-wise.\n",
        "\n",
        "Here are any formulas and further details you might need:\n",
        "- True positive means that the output and prediction match, and they are both true.\n",
        "- True negative means that the output and prediction match, and they are both false.\n",
        "- False positive means that the output and prediction do not match, and the prediction is true.\n",
        "- False negative means that the output and prediction do not match, and the prediction is false.\n",
        "- The formula for precision is:\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$\\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "- The formula for recall is:\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$\\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "- The formula for the F1 Score is:\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$2 * \\,\\frac{\\text{Precision}*\\text{Recall}}{\\text{Precision}+\\text{Recall}}$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "Your final F1 Score should be slightly higher than 80%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpwFwdqUroyM"
      },
      "outputs": [],
      "source": [
        "predictions = (f(thetas, validationData) > 0.5).flatten()\n",
        "outputs = (validationOutputs == 1).flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ON7IENMMUE0"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example program that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "predictions = (f(thetas, validationData) > 0.5).flatten()\n",
        "outputs = (validationOutputs == 1).flatten()\n",
        "\n",
        "tp = np.sum(outputs & predictions)\n",
        "fp = np.sum(~outputs & predictions)\n",
        "tn = np.sum(~outputs & ~predictions)\n",
        "fn = np.sum(outputs & ~predictions)\n",
        "\n",
        "print(f\"True Positives: {tp}\")\n",
        "print(f\"True Negatives: {tn}\")\n",
        "print(f\"False Positives: {fp}\")\n",
        "print(f\"False Negatives: {fn}\")\n",
        "\n",
        "precision = tp/(tp+fp)\n",
        "recall = tp/(tp+fn)\n",
        "\n",
        "print(f\"\\nPrecision: {np.round(precision, 2)}\")\n",
        "print(f\"Recall: {np.round(recall, 2)}\")\n",
        "\n",
        "print(f\"\\nF1 Score: {np.round(2 * precision*recall / (precision+recall) * 100, 2)}%\")\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MXvlf8YGuv-"
      },
      "source": [
        "## Credits\n",
        "* This lab used a modified version of Naresha Bhat's heart attack possibility dataset from Kaggle. You can find the original dataset [here](https://www.kaggle.com/datasets/nareshbhat/health-care-data-set-on-heart-attack-possibility).\n",
        "* Formula to calculate the F1 score from [Wikipedia](https://en.wikipedia.org/wiki/F-score)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "ce9e5fc50149dfb09ca60d84ae2680dbf18b569ea9aa8de02c26aa2ebff8fc36"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
