{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMvSWi9vkuwe"
      },
      "source": [
        "# Training Faster and Better Lab\n",
        "## Preprocessing Data\n",
        "### Import The Data\n",
        "Much like the last lab, we need to import the dataset. The last lab did this for you automatically, it's time to take off the training wheels. Your job is to import the data from the CSV file from [this github link](https://raw.githubusercontent.com/Endothermic-Dragon/Polygence/master/Jupyter%20Notebooks/Training%20Faster%20and%20Better/House%20Pricing%20Dataset.csv). Feel free to refer to the code from the last lab, but here's a step-by-step explanation of what you have to do:\n",
        "1. Import numpy as np, so you can use it to work with large arrays and matrices.\n",
        "2. Import pandas as pd, so you can use it to fetch the data and display the data table to confirm that all the data has been fetched correctly.\n",
        "3. Use `pd.read_csv()` to fetch the data from the appropriate URL.\n",
        "  - [Click for documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)\n",
        "  - You only need to input the `filepath_or_buffer` parameter - essentially, you should only have one input into the `pd.read_csv()` function.\n",
        "  - Assign the output of the function to the variable `dataTable`.\n",
        "4. To display the data table, simply type in `dataTable` on an empty line at the end\n",
        "A template has set up so it's easier to fill in the proper code. Once again, feel free to refer to the code from the last lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojpVXQvSkmdj"
      },
      "outputs": [],
      "source": [
        "# Import numpy as np, and pandas as pd\n",
        "\n",
        "# Get data from CSV file on GitHub\n",
        "\n",
        "# Display data table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4PaB_3eFW9Q"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example program that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "# Deal with large arrays quickly and easily\n",
        "import numpy as np\n",
        "# Display data table\n",
        "import pandas as pd\n",
        "\n",
        "# Get data from CSV file on GitHub\n",
        "dataTable = pd.read_csv(\"https://raw.githubusercontent.com/Endothermic-Dragon/Polygence/master/Jupyter%20Notebooks/Training%20Faster%20and%20Better/House%20Pricing%20Dataset.csv\")\n",
        "\n",
        "# Display data table\n",
        "dataTable\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDnju-yTB8IT"
      },
      "source": [
        "Our goal with all of these variables is to predict the data for \"House price of unit area\".\n",
        "\n",
        "### Visualize The Data\n",
        "\n",
        "Now that you've imported the data, you want to visualize all of it. A good library to do this is `seaborn`, which works hand-in-hand with matplotlib's `pyplot`. Run the cell below to visualize all variable relationships and hisograms. Keep in mind that this does take from about 25 to 45 seconds to render, because of all the graphs and variables involved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToLmQ-3VqBNz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create and show plot\n",
        "sns.pairplot(dataTable)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "untfK18SDfqq"
      },
      "source": [
        "### Analyzing correlations\n",
        "\n",
        "Looking through the data carefully, we can make a few observations;\n",
        "1. The data for `Distance to the nearest MRT station` is very skewed\n",
        "    - Apply a nonlinear transformation, such as a natural log\n",
        "2. `Latitude` and `Longitude` don't seem to have a specific relationship with `House price of unit area`\n",
        "    - This could hint that the relationship is not exactly visible in two dimensions, but might have a correlation in higher dimensions\n",
        "    - We would generally expect latitude and longitude to affect the house price in some way as it makes sense, but how they're related is unknown to us\n",
        "    - We want to bucket this data so our algorithm can model nonlinear relationships\n",
        "3. The scatterplot for `Longitude` and `Distance to the nearest MRT station` is very linear, but has a very sharp break\n",
        "    - We want to make sure to insert a boundary for a bin there\n",
        "4. There seems to be an outlier for `House price of unit area`\n",
        "\n",
        "We've visualized our data and drew some basic conclusions. Now, let's visualize the correlation coefficients. Pandas has a built-in function to calculate the matrix, and we can feed it to seaborn to visualize it as a heatmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6WOB8U7rHY7"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8,7))\n",
        "\n",
        "# Get the correlation matrix, and round to two decimals to visualize sparsely\n",
        "corr = dataTable.corr().round(2)\n",
        "\n",
        "# Display the heatmap of the matrix\n",
        "sns.heatmap(corr, annot=True, ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dltDsg3eKPfx"
      },
      "source": [
        "### Further Analysis and Transforming Data\n",
        "\n",
        "We can see from the heatmap that `Distance to the nearest MRT station` has a pretty decent relationship with `House price of unit area`. However, previously we saw that the data was skewed. Let's see if distributing the data more evenly helps increase the level of correlation.\n",
        "\n",
        "The space below is for you to experiment for what transformations work best.\n",
        "\n",
        "<details>\n",
        "<summary>What I did</summary>\n",
        "\n",
        "`np.log(distance)**1.1`\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZUQzStFKDrT"
      },
      "outputs": [],
      "source": [
        "# Get all the data from the \"Distance to the nearest MRT station\" column\n",
        "distance = dataTable[\"Distance to the nearest MRT station\"]\n",
        "\n",
        "# Modify the values so it looks more like a normal distribution\n",
        "# EDIT HERE\n",
        "distanceModified = distance\n",
        "\n",
        "# Plot the old relationship\n",
        "sns.histplot(distance)\n",
        "plt.show()\n",
        "\n",
        "# Plot the new relationship\n",
        "sns.histplot(distanceModified)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHQu-o3lvbMF"
      },
      "source": [
        "Once you're happy with your transformed data, run the next cell to replace the old data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MzdAvWPcx5i"
      },
      "outputs": [],
      "source": [
        "# Reassign the column data with the modified data\n",
        "dataTable[\"Distance to the nearest MRT station\"] = distanceModified"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a5FwVRwvuhb"
      },
      "source": [
        "Now, check the fruit of your labors. Run the next cell to regraph the correlation matrix. The value's <u>*magnitude*</u> should have gone up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPMUNmwei9y2"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8,7))\n",
        "\n",
        "corr = dataTable.corr().round(2)\n",
        "sns.heatmap(corr, annot=True, ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJfWutzFwTq8"
      },
      "source": [
        "Let's address the relationship between `Longitude` and `Distance to the nearest MRT station`. Since we'll be making buckets for `Longitude`, we can simply identify the boundary. Keep in mind that the graph won't be linear anymore as we just applied various transformations to `Distance to the nearest MRT station`, but the boundary should still be clear. Once identifying the critical section you want to see in more detail, zoom into that area.\n",
        "\n",
        "After zooming in, make sure to take note of the bottom right of the graph, which shows how shifted the graph is. In the context of computer science, `e` represents to what power of 10 the number before it is scaled. For example, `1.7e+3` actually means $1.7 \\cdot 10^3$. A negative value after the `e` represents a negative exponent. For example, `0.8e-13` actually means $0.8 \\cdot 10^{-13}$.\n",
        "\n",
        "<details>\n",
        "<summary>What I did</summary>\n",
        "\n",
        "These bounds worked well with the transformation I used: `[121.53, 121.55, 5, 8.5]`\n",
        "\n",
        "Reguardless of what transformation you used, you should get a boundary value close to 121.54125.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnaooep0tVyh"
      },
      "outputs": [],
      "source": [
        "# Plot the data\n",
        "dataTable.plot(kind='scatter', x='Longitude', y='Distance to the nearest MRT station')\n",
        "plt.show()\n",
        "\n",
        "# Zoom in to the area of change to pinpoint where \n",
        "dataTable.plot(kind='scatter', x='Longitude', y='Distance to the nearest MRT station')\n",
        "# Set zoomed area bounds\n",
        "# EDIT HERE\n",
        "# The parameters in the array should be numbers (floats to be specific), not strings\n",
        "plt.axis([\"x lower limit\", \"x higher limit\", \"y lower limit\", \"y higher limit\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flKE-6dJ8Gfg"
      },
      "source": [
        "For now, remember that value (or scroll back up later). We'll deal with this again in the near future.\n",
        "\n",
        "From our correlation matrix, we can see that the following values are somewhat low, when compared to `House price of unit area`:\n",
        "- `Number of convenience stores`\n",
        "- `House age`\n",
        "- `Transaction date`\n",
        "\n",
        "So, let's graph them to see if there's anything we can do.\n",
        "\n",
        "We'll start with `Number of convenience stores`. Since `Number of convenience stores` is going to be an input in our model, and the output is supposed to predict `House price of unit area`, we'll have `Number of convenience stores` be on the x-axis and `House price of unit area` be on the y-axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLb35EyNs7r-"
      },
      "outputs": [],
      "source": [
        "dataTable.plot(kind='scatter', x='Number of convenience stores', y='House price of unit area')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Zzz_BH__-c"
      },
      "source": [
        "We can see that it roughly does have a linear shape, so there's not much we can do about that. However, we see the same outlier we saw earlier! Let's remove it.\n",
        "\n",
        "We can seperate this by its value of `House price of unit area`. The other datapoints are around 45ish, but this point is at around 120. Visually, we can see that 100 is a clear cutoff mark that seperates this from all the other points. Use this observation to select ONLY the outlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPZa1Vyw1bP6"
      },
      "outputs": [],
      "source": [
        "# Conditionally select the outlier point\n",
        "# EDIT HERE\n",
        "# Change the condition so it only satisfied the outlier point\n",
        "droppedData = dataTable['House price of unit area'] != 0\n",
        "\n",
        "# Get all the indexs of the data you're dropping (should be ONLY the outlier)\n",
        "index = dataTable[droppedData].index\n",
        "\n",
        "if len(index) > 1:\n",
        "    print(\"Oops, there's an error. Here's what you selected:\")\n",
        "    print(index)\n",
        "else:\n",
        "    print(f\"You've selected the datapoint at index {index[0]}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzJX7oOLi_ZR"
      },
      "source": [
        "Now, delete this datapoint by running the next line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlUrhIcVjD-N"
      },
      "outputs": [],
      "source": [
        "dataTable = dataTable.drop(index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va8OeH1sjrWl"
      },
      "source": [
        "Next, let's tackle `House age` in a similar manner - by graphing it on a scatterplot!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9KEa6qLw6yf"
      },
      "outputs": [],
      "source": [
        "dataTable.plot(kind='scatter', x='House age', y='House price of unit area')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrc4dPTAyUon"
      },
      "source": [
        "There's a pretty clear nonlinear relationship where it starts high, dips down, and goes back up again. A quadratic fit would suffice for this. We'll add a `House price of unit area^2` feature later on to account for this.\n",
        "\n",
        "For now, let's move on to `Transaction date`. The date is in a decimal format. Let's turn that into years, months, and days, as you would generally expect different trends that vary by season (and thus, by time). The input for the function will be a 1D row of dates expressed as a year (but in decimal form). Why is it a row, you ask? That's because when a specific column is retrieved from `dataTable`, it returns a 1D array. The job of your function will be to take that, and output a list of 3 arrays - the first one being the integer year, the second one being the integer month, and the third one being the integer date. To be honest, you can most likely treat the input array as a decimal input, as `numpy` automatically applies that operation to all its elements.\n",
        "\n",
        "Some helpful operators you might need:\n",
        "- `/` - plain old division, returns decimal (float) result\n",
        "- `//` - division ignore remainder (returns quotient)\n",
        "- `%` - division ignore quotient, also called a modulus (returns remainder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMHOJQmPyeVn"
      },
      "outputs": [],
      "source": [
        "# EDIT HERE\n",
        "def breakDate(years):\n",
        "    yearsReturn = \n",
        "    monthsReturn = \n",
        "    daysReturn = \n",
        "    return [yearsReturn, monthsReturn, daysReturn]\n",
        "\n",
        "# Use a test input and an if statement to confirm correct implementation\n",
        "testInput = breakDate(np.array([[3 + 5/12 + 17/12/30]]))\n",
        "\n",
        "# Account for float error and implementation variation\n",
        "if testInput[0][0,0] == 3 and testInput[1][0,0] == 5 and testInput[2][0,0] in [16, 17]:\n",
        "    # \"Spread\" list output and assign to three variables\n",
        "    years, months, days = breakDate(dataTable[\"Transaction date\"])\n",
        "\n",
        "    # Add new columns to dataTable\n",
        "    dataTable[\"Year\"] = years\n",
        "    dataTable[\"Month\"] = months\n",
        "    dataTable[\"Day\"] = days\n",
        "\n",
        "    # Remove the old measure\n",
        "    dataTable = dataTable.drop(labels=\"Transaction date\", axis=1)\n",
        "else:\n",
        "    print(\"Oops, looks like your implementation is incorrect!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuQgi-zDQ3so"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example function that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "def breakDate(years):\n",
        "    yearsReturn = years // 1\n",
        "    monthsReturn = ((years % 1) * 12) // 1\n",
        "    daysReturn = ((((years % 1) * 12) % 1) * 30) // 1\n",
        "    return [yearsReturn, monthsReturn, daysReturn]\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XydsP1ZTrPol"
      },
      "source": [
        "Next, let's visualize the data we just transformed. One of the lines is set up for you as an example, fill in the other two lines. Make sure to take not of the function name and parameter inputs, as you'll have to do this by yourself sooner or later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiC5isIXzln6"
      },
      "outputs": [],
      "source": [
        "# Display scatterplot for year\n",
        "dataTable.plot(kind='scatter', x='Year', y='House price of unit area')\n",
        "plt.show()\n",
        "\n",
        "# Display scatterplot for month\n",
        "\n",
        "# Display scatterplot for day"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sKPdzCVSJEJ"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example program that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "# Display scatterplot for month\n",
        "dataTable.plot(kind='scatter', x='Month', y='House price of unit area')\n",
        "plt.show()\n",
        "\n",
        "# Display scatterplot for day\n",
        "dataTable.plot(kind='scatter', x='Day', y='House price of unit area')\n",
        "plt.show()\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjg2gl2u0Bre"
      },
      "source": [
        "From the above information, we can conclude a few things:\n",
        "- There is an overall deviation in the price from the year 2021 to 2013. This is important data to keep, as it will provide our model with an overall sense of trends.\n",
        "- There is a slight fluctuation over the months. I personally feel that a cubic should be enough to get the gist of the trend, but if you feel otherwise, feel free to use your own adaptation!\n",
        "- The days recorded are either at the beginning of the month or at the end. It doesn't really provide any extra information; nor are there any large-scale general patterns to help train our model.\n",
        "\n",
        "As per this analysis, let's strike out `Day`. We'll modify `Month` later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVgnERv20X7O"
      },
      "outputs": [],
      "source": [
        "# The axis parameter tells the dataframe that we're trying to strike out a column\n",
        "dataTable = dataTable.drop(\"Day\", axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkiYQEJX0aey"
      },
      "source": [
        "Great, now we finished identifying what we need to change (and even changed a few)!\n",
        "\n",
        "### Binning and Adding Preprocessed Features\n",
        "\n",
        "So far we have:\n",
        "- Create bins for latitude\n",
        "- Create bins for longitude, making sure to add a bin for 121.54125\n",
        "- House age - fit with a quadratic\n",
        "  - Add feature which is squared\n",
        "- Month - fit with a cubic\n",
        "  - Add features which are squared and cubed\n",
        "\n",
        "Before making bins, let's first visualize the histograms of `Latitude` and `Longitude`. Use `sns.histplot()` to do this. Make sure to specify what you're plotting as the `x` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaDWg-8SsbrD"
      },
      "outputs": [],
      "source": [
        "# Histogram for latitude\n",
        "sns.histplot(dataTable, x=\"Latitude\")\n",
        "plt.show()\n",
        "\n",
        "# Histogram for longitude\n",
        "sns.histplot(dataTable, x=\"Longitude\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7hH1fV6uZ9L"
      },
      "source": [
        "Now, let's make the buckets by quantile (also known as percentile). One of them is shown, your job is to code the other one. Make sure to not just copy and paste, but to truly *understand* what you're coding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qetKDn1vUREU"
      },
      "outputs": [],
      "source": [
        "# Get the boundary values, add -infinity and infinity to represent open intervals on either end\n",
        "latitudeBins = np.append(np.percentile(dataTable.Latitude, np.arange(1,10)*10), [-np.Inf, np.Inf])\n",
        "# Sort to get the infinities at the right spots\n",
        "latitudeBins.sort()\n",
        "# Use panda's cut function to bin and automatically label each datapoint\n",
        "latitudeData = pd.cut(dataTable.Latitude, bins=latitudeBins, labels=[\"Latitude Bin \" + str(i) for i in range(1, 11)])\n",
        "# Create bin column, assign 1 if in bin, otherwise assign 0\n",
        "for i in range(1,11):\n",
        "    binName = \"Latitude Bin \" + str(i)\n",
        "    dataTable[binName] = (latitudeData == binName).astype(int)\n",
        "\n",
        "# EDIT HERE\n",
        "# Get the boundary values, add -infinity and infinity to represent open intervals on either end\n",
        "# Also add 121.54125 as a bin boundary\n",
        "\n",
        "# Sort to get the infinities and 121.54125 at the right spots\n",
        "\n",
        "# Use panda's cut function to bin and automatically label each datapoint\n",
        "# Take not that there's now one more bin than there was before\n",
        "\n",
        "# Create bin column, assign 1 if in bin, otherwise assign 0\n",
        "# Take not that there's now one more bin than there was before\n",
        "\n",
        "\n",
        "# Delete the latitude and longitude data, as we don't need it anymore\n",
        "dataTable = dataTable.drop(labels=[\"Latitude\", \"Longitude\"], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_2IvoPxS5Oq"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example program that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "# Get the boundary values, add -infinity and infinity to represent open intervals on either end\n",
        "# Also add 121.54125 as a bin boundary\n",
        "longitudeBins = np.append(np.percentile(dataTable.Longitude, np.arange(1,10)*10), [-np.Inf, 121.54125, np.Inf])\n",
        "# Sort to get the infinities and 121.54125 at the right spots\n",
        "longitudeBins.sort()\n",
        "# Use panda's cut function to bin and automatically label each datapoint\n",
        "# Take not that there's now one more bin than there was before\n",
        "longitudeData = pd.cut(dataTable.Longitude, bins=longitudeBins, labels=[\"Longitude Bin \" + str(n) for n in range(1, 12)])\n",
        "# Create bin column, assign 1 if in bin, otherwise assign 0\n",
        "# Take not that there's now one more bin than there was before\n",
        "for i in range(1,12):\n",
        "    binName = \"Longitude Bin \" + str(i)\n",
        "    dataTable[binName] = (longitudeData == binName).astype(int)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT63xdiQf08q"
      },
      "source": [
        "Now, let's create the preprocessed features for `House age` and `Month`. I used just squared for `House age`, and squared and cubed for `Month`. If you want to use something different, simply modify the code below! Otherwise, just run the cell.\n",
        "\n",
        "Once again, we precompute this beforehand so we don't have to recalculate it on every iteration, and can simply fit values of θ to it - plus, it makes dealing with derivatives and matrices easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SX8G2by_4Wwo"
      },
      "outputs": [],
      "source": [
        "# Add the other features\n",
        "dataTable[\"House age^2\"] = dataTable[\"House age\"]**2\n",
        "dataTable[\"Month^2\"] = dataTable[\"Month\"]**2\n",
        "dataTable[\"Month^3\"] = dataTable[\"Month\"]**3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAtjSuWC-EMk"
      },
      "source": [
        "Next, we to Z-Score all the data for efficient training. Fill in the `zScore` funciton, once again keeping in mind the input is a 1D row of data (while technically we're singling out a column, getting the data column from `dataTable` returns a 1D array). The formula to calculate a Z-Score is $x^{\\prime} = (x - μ) / σ$, where μ is the mean and σ is the standard deviation.\n",
        "\n",
        "You can use `np.mean(a)` to calculate the mean of all elements in array `a`. You can also calculate the standard deviation of all elements in array `a` with `np.std(a)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmLFUXA65ock"
      },
      "outputs": [],
      "source": [
        "# EDIT HERE\n",
        "def zScore(columnData):\n",
        "    return\n",
        "\n",
        "# Use a test input and an if statement to confirm correct implementation\n",
        "testInput = np.array([1,2,3,4,5])\n",
        "testOutput = zScore(testInput)\n",
        "\n",
        "# Account for float error while checking\n",
        "if -0.01 < testOutput[0]+1.414 < 0.01 and -0.01 < testOutput[2] < 0.01:\n",
        "    # Apply Z-Score function\n",
        "    for i in dataTable.columns:\n",
        "        # Exclude our bins\n",
        "        if \"Latitude\" not in i and \"Longitude\" not in i:\n",
        "            dataTable[i] = zScore(dataTable[i])\n",
        "\n",
        "    # Reorder the data columns and remove any other unnecessary columns if we didn't already\n",
        "    dataTable = dataTable[[\n",
        "                        \"Number of convenience stores\",\n",
        "                        \"Distance to the nearest MRT station\",\n",
        "                        \"House age\",\n",
        "                        \"House age^2\",\n",
        "                        \"Year\",\n",
        "                        \"Month\",\n",
        "                        \"Month^2\",\n",
        "                        \"Month^3\",\n",
        "                        *[\"Latitude Bin \" + str(i) for i in range(1, 11)],\n",
        "                        *[\"Longitude Bin \" + str(n) for n in range(1, 12)],\n",
        "                        \"House price of unit area\",\n",
        "                        ]]\n",
        "\n",
        "    # Display the data table\n",
        "    display(dataTable)\n",
        "else:\n",
        "    print(\"Oops, you have an incorrect implementation of calculating the Z-Score!\\nRecheck your code.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuQwU-KfTY3N"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example function that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "def zScore(columnData):\n",
        "    return (columnData - np.mean(columnData)) / np.std(columnData)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B89TvjvBUVf"
      },
      "source": [
        "### Finishing Up Preprocessing\n",
        "\n",
        "Great! Now we have all of our data preprocessed! Now, we just need to randomize our data and split into training and validation sets. Note that a column of 1s is added to the training and validation data. This acts as the bias term - any coefficient that is multiplied with it will be returned as-is, and will be added to the other terms.\n",
        "\n",
        "Doing this step is trivial since we're focusing on the mathematics, so it is done for you already. However, it is still recommended that you look through the code to understand what it is doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EhDp8IT9qpk"
      },
      "outputs": [],
      "source": [
        "# Randomly scamble data, but in a consistent manner (random_state is kind of like a random seed)\n",
        "dataTable = dataTable.sample(frac=1, random_state=314).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUCBQjXS7LOT"
      },
      "outputs": [],
      "source": [
        "# If odd number of data points, include in training data\n",
        "splitPoint = len(dataTable) // 2 + len(dataTable) % 2\n",
        "\n",
        "trainingData = dataTable.drop(labels=\"House price of unit area\", axis=1).to_numpy()[:splitPoint]\n",
        "trainingData = np.hstack([\n",
        "                          np.ones((splitPoint, 1)),\n",
        "                          trainingData\n",
        "                         ])\n",
        "trainingOutputs = dataTable[[\"House price of unit area\"]].to_numpy()[:splitPoint]\n",
        "\n",
        "validationData = dataTable.drop(labels=\"House price of unit area\", axis=1).to_numpy()[splitPoint:]\n",
        "validationData = np.hstack([\n",
        "                          np.ones((len(dataTable) - splitPoint, 1)),\n",
        "                          validationData\n",
        "                           ])\n",
        "validationOutputs = dataTable[[\"House price of unit area\"]].to_numpy()[splitPoint:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb8e3rjSPrMh"
      },
      "source": [
        "The code below has been set up for you so you simply have to fill in the functions. This should be pretty doable, given that you did a similar activity last lab.\n",
        "\n",
        "This time, the `thetas` is a vector-like vertical matrix/array.\n",
        "\n",
        "Also... new numpy function! The `@` function performs the dot product of two numpy arrays. This *might* be helpful (hint hint) when batch calculating `f`, which should take in all the data and the values of θ, and output a column of predictions.\n",
        "\n",
        "Calculating `J` should be simple enough, given the formula:\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$J(θ) = \\frac{1}{n} \\sum_{i=1}^n (f(θ, x_i) - y_i)^2$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "and the `np.mean(...)` function.\n",
        "\n",
        "The `getGradients` function is a bit more difficult though. From the past article, we considered each row one-by-one. Specifically, for each row, we took the data for that row, plugged it into `f` (along with the values of $\\theta$), and multiplied it to each feature before putting it in a list. At the end, we took the average of each feature list and multiplied it by 2, before returning it as an array.\n",
        "\n",
        "You can do the same operations, but now a lot more efficient and organized manner. First, you have to plug in the entire matrix of data and the θ values into `f`. As per previous specifications, you know that `f` will return a column of outputs. You can directly multiply this with `*`, as numpy is \"smart\" enough to figure out to multiply each output row (consisting of one element) with each feature row (consisting of multiple elements), and in the process \"stretch\" the output row to match the size of the feature row (by repeating the singular element). This is called broadcasting, and you can read more about it [here](https://numpy.org/doc/stable/user/basics.broadcasting.html). After that, take the mean of the array, but while doing so, make sure to specify the axis, which specifies which direction to \"collapse\" the data in when taking the mean. In this example, your axis should be 0, which takes the mean \"vertically\" - in other words, it returns the mean of each column. For the sake of completeness, you should also be aware that an axis of 1 corresponds to taking the mean \"horizontally\", and returns the mean of each row (as you would expect). Note that since this process \"collapses\" by a dimension, it returns a 1D array, despite taking a 2D array as input. You need to turn into a vertical 2D array to be compatible with the array shape of `thetas`. Finally, multiply all the values of the array by 2, transform into a 2D vertical array, and return it.\n",
        "\n",
        "<details>\n",
        "<summary>Hint to transform into 2D vertical array</summary>\n",
        "\n",
        "To transform a 1D array into a vertical 2D array, you might find the `reshape` numpy function useful. You can find its documentation [here](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html). However, there are other methods. For example, you can use `np.newaxis`. Or, my personal favorite (due to its readability and simplicity), wrap it in a list, convert to an array, and use `a.T`, with `a` as your array, to \"flip\" the arrays's elements - formally known as a matrix transposition.\n",
        "</details>\n",
        "\n",
        "Phew! That was a lot. While initially it may require a bit of effort to understand and implement all of that, in the end your code should be about 3 to 4 lines - a huge improvement in readability. In addition, numpy is a library programmed partially in C, which is much faster than regular python. Consequently, your code should run much faster, too. If you want to know why this is the case, check out [this article](https://www.huffpost.com/entry/computer-programming-languages-why-c-runs-so-much_b_59af8178e4b0c50640cd632e).\n",
        "\n",
        "The code in all of your functions should be very clear and concise, and should not utilize any loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOc3iV-a5lk4"
      },
      "outputs": [],
      "source": [
        "thetas = np.zeros([\n",
        "                    trainingData.shape[1],\n",
        "                    1\n",
        "                  ])\n",
        "\n",
        "def f(thetas, data=trainingData):\n",
        "    return\n",
        "\n",
        "def J(thetas, data=trainingData, outputs=trainingOutputs):\n",
        "    return\n",
        "\n",
        "def getGradients(thetas, data=trainingData, outputs=trainingOutputs):\n",
        "    return\n",
        "\n",
        "iterations = 1000\n",
        "cost_history = []\n",
        "cost_history.append(J(thetas))\n",
        "for i in range(iterations):\n",
        "    # Learning rate is chosen for you, but feel free to modify\n",
        "    # If you get any errors involving infinity or your cost is increasing, chances are the learning rate is too high\n",
        "    thetas = thetas - 0.25 * getGradients(thetas)\n",
        "    cost_history.append(J(thetas))\n",
        "\n",
        "print(\"Initial cost:\", cost_history[0])\n",
        "print(\"Final cost:\", cost_history[-1])\n",
        "plt.plot(np.arange(iterations+1), cost_history)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORllyf4ePvv5"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example program that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "thetas = np.zeros([\n",
        "                    trainingData.shape[1],\n",
        "                    1\n",
        "                  ])\n",
        "\n",
        "def f(thetas, data=trainingData):\n",
        "    return data @ thetas\n",
        "\n",
        "def J(thetas, data=trainingData, outputs=trainingOutputs):\n",
        "    return np.sum((f(thetas, data) - outputs)**2) / len(data)\n",
        "\n",
        "def getGradients(thetas, data=trainingData, outputs=trainingOutputs):\n",
        "    resultPart = f(thetas, data) - outputs\n",
        "    results = 2 * np.mean(resultPart * data, 0)\n",
        "\n",
        "    return np.array([results]).T\n",
        "\n",
        "iterations = 1000\n",
        "cost_history = []\n",
        "cost_history.append(J(thetas))\n",
        "for i in range(iterations):\n",
        "    thetas = thetas - 0.25 * getGradients(thetas)\n",
        "    cost_history.append(J(thetas))\n",
        "\n",
        "print(\"Initial cost:\", cost_history[0])\n",
        "print(\"Final cost:\", cost_history[-1])\n",
        "plt.plot(np.arange(iterations+1), cost_history)\n",
        "plt.show()\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dy32KTGOTEJ"
      },
      "source": [
        "In the next cell, fill in the missing code to visualize the cost of the validation data and ensure that you're not overfitting. The code from the last cell or last lab might be useful. Note that the functions you previously wrote carry over in memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNtaXGY89Pzx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaIcxRCBioXK"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example program that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "thetas = np.zeros([\n",
        "                   trainingData.shape[1],\n",
        "                   1\n",
        "                  ])\n",
        "\n",
        "iterations = 1000\n",
        "cost_history = []\n",
        "cost_history.append(J(thetas, validationData, validationOutputs))\n",
        "for i in range(iterations):\n",
        "    thetas = thetas - 0.25 * getGradients(thetas)\n",
        "    cost_history.append(J(thetas, validationData, validationOutputs))\n",
        "\n",
        "print(\"Initial cost:\", cost_history[0])\n",
        "print(\"Final cost:\", cost_history[-1])\n",
        "plt.plot(np.arange(iterations+1), cost_history)\n",
        "plt.show()\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diwFCuZAOhVA"
      },
      "source": [
        "Finally, calculate the accuracy of your model. Remember, the formula is as follows:\n",
        "\n",
        "<h3>\n",
        "\n",
        "$$R^2 = 1-\\frac{\\sum_{i=1}^n (f(\\theta, x_i) - y_i)^2}{\\sum_{i=1}^n (y_{average} - y_i)^2}$$\n",
        "\n",
        "</h3>\n",
        "\n",
        "You should use `np.mean(...)` to calculate the average, and `np.sum(...)` to calculate the sums of the errors. In total, your code should have around 5-7 lines of code (excluding comments and empty lines), and you should not have any loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHs4VMNp9VbO"
      },
      "outputs": [],
      "source": [
        "# Value of thetas carry over from previous cell\n",
        "\n",
        "# Get the mean of the output values\n",
        "\n",
        "# Calculate numerator and denominator sum with loop\n",
        "\n",
        "# Plug into formula\n",
        "\n",
        "# Display R^2 as percentage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXdiA4Dui0v7"
      },
      "source": [
        "<details>\n",
        "<summary>Stuck or completed? Click here to reveal a working example program that you could've written.</summary>\n",
        "\n",
        "```python\n",
        "# Value of thetas carry over from previous cell\n",
        "\n",
        "# Get the mean of the output values\n",
        "averageValidationOutput = np.mean(validationOutputs)\n",
        "\n",
        "# Calculate numerator and denominator sum with loop\n",
        "numerator = np.sum((f(thetas, validationData) - validationOutputs)**2)\n",
        "denominator = np.sum((validationOutputs - averageValidationOutput)**2)\n",
        "\n",
        "# Plug into formula\n",
        "r_squared = 1 - numerator/denominator\n",
        "\n",
        "# Display R^2 as percentage\n",
        "print(f\"{'%.2f' % (r_squared*100)}% accuracy\")\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1_wK4xQE4SN"
      },
      "source": [
        "## Credits\n",
        "* This lab used a modified version of Algor_Bruce's real estate price prediction dataset from Kaggle. You can find the original dataset [here](https://www.kaggle.com/datasets/quantbruce/real-estate-price-prediction).\n",
        "* Formula to calculate $R^2$ accuracy from [Newcastle University, UK](https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/coefficient-of-determination-r-squared.html)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "ce9e5fc50149dfb09ca60d84ae2680dbf18b569ea9aa8de02c26aa2ebff8fc36"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
